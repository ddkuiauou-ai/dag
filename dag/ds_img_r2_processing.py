"""
DS í”„ë¡œì íŠ¸ R2 ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
PNG íŒŒì¼ì„ WebP í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ë¥¸ ë²„í‚·ì— ì €ì¥
"""

import os
import io
import re
import uuid
from typing import List, Dict, Any
from datetime import datetime
import time  # Added import
from dataclasses import dataclass
from pathlib import Path

import boto3
from PIL import Image
import dagster as dg
import duckdb
import psycopg2
from psycopg2.extras import RealDictCursor

from dagster_duckdb import DuckDBResource
from .resources import PostgresResource




# â”€â”€â”€â”€ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ â”€â”€â”€â”€
R2_ACCESS_KEY_ID = os.getenv("DS_R2_ACCESS_KEY_ID")
R2_SECRET_ACCESS_KEY = os.getenv("DS_R2_SECRET_ACCESS_KEY")
R2_ENDPOINT = os.getenv("DS_R2_ENDPOINT")
R2_SOURCE_BUCKET = os.getenv("DS_R2_SOURCE_BUCKET")  # PNG íŒŒì¼ì´ ìˆëŠ” ì†ŒìŠ¤ ë²„í‚·
R2_TARGET_BUCKET = os.getenv("DS_R2_TARGET_BUCKET")  # WebP íŒŒì¼ì„ ì €ì¥í•  íƒ€ê²Ÿ ë²„í‚·
DS_PUBLIC_URL = os.getenv("DS_PUBLIC_URL")  # ê³µê°œ URL


# â”€â”€â”€â”€ DS ì´ë¯¸ì§€ ì²˜ë¦¬ ê²½ë¡œ ê´€ë¦¬ â”€â”€â”€â”€
@dataclass
class DSImagePaths:
    """DS ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì¼ ê²½ë¡œ ê´€ë¦¬"""
    base_dir: Path = Path("data") / "images" / "ds"
    
    @property
    def source_png_dir(self) -> Path:
        """R2ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ PNG íŒŒì¼ ì„ì‹œ ì €ì¥ ë””ë ‰í† ë¦¬"""
        return self.base_dir / R2_SOURCE_BUCKET
        
    @property
    def converted_webp_dir(self) -> Path:
        """ë³€í™˜ëœ WebP íŒŒì¼ ì„ì‹œ ì €ì¥ ë””ë ‰í† ë¦¬"""
        return self.base_dir / R2_TARGET_BUCKET
    
    def ensure_directories(self) -> None:
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        self.source_png_dir.mkdir(parents=True, exist_ok=True)
        self.converted_webp_dir.mkdir(parents=True, exist_ok=True)

# DS ì´ë¯¸ì§€ ê²½ë¡œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
ds_image_paths = DSImagePaths()



def get_r2_client():
    """R2 í´ë¼ì´ì–¸íŠ¸ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    if not all([R2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, R2_ENDPOINT]):
        raise ValueError("R2 ìê²© ì¦ëª…ì´ .env íŒŒì¼ì— ì™„ì „íˆ êµ¬ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    return boto3.client(
        service_name='s3',
        aws_access_key_id=R2_ACCESS_KEY_ID,
        aws_secret_access_key=R2_SECRET_ACCESS_KEY,
        endpoint_url=R2_ENDPOINT,
        region_name='auto'
    )


@dg.asset(
    group_name="DS",
    kinds={"source"},
    deps=["ds_img_r2_docker", "ds_img_r2_node"],  
    tags={
        "domain": "image_processing",
        "data_tier": "bronze", 
        "source": "cloudflare_r2"
    },
    description="DS í”„ë¡œì íŠ¸ - R2 ì†ŒìŠ¤ ë²„í‚·ì—ì„œ PNG íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ì— ì €ì¥ (Bronze Tier)"
)
def ds_r2_download(context: dg.AssetExecutionContext) -> dg.MaterializeResult:
    """
    R2 ì†ŒìŠ¤ ë²„í‚·ì—ì„œ PNG íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    ë¡œì»¬ì— ì €ì¥ëœ PNG íŒŒì¼ ê²½ë¡œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not R2_SOURCE_BUCKET:
        raise ValueError("R2_SOURCE_BUCKETì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    ds_image_paths.ensure_directories()
    for existing_file in ds_image_paths.source_png_dir.glob("*.png"):
        existing_file.unlink()

    s3_client = get_r2_client()
    downloaded_files = []
    all_png_objects = []
    
    total_download_time_seconds = 0.0 # Initialize here for scope

    try:
        context.log.info(f"R2 ì†ŒìŠ¤ ë²„í‚· '{R2_SOURCE_BUCKET}'ì—ì„œ PNG íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        paginator = s3_client.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=R2_SOURCE_BUCKET):
            if "Contents" in page:
                for obj in page["Contents"]:
                    if obj["Key"].lower().endswith(".png"):
                        all_png_objects.append(obj)
        
        total_files = len(all_png_objects)

        if total_files == 0:
            context.log.info("ë‹¤ìš´ë¡œë“œí•  PNG íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return dg.MaterializeResult(
                metadata={
                    "dagster/row_count": dg.MetadataValue.int(0),
                    "source_bucket": dg.MetadataValue.text(R2_SOURCE_BUCKET),
                    "downloaded_files_count": dg.MetadataValue.int(0),
                    "total_files_in_bucket_png": dg.MetadataValue.int(0),
                    "total_download_time_seconds": dg.MetadataValue.float(0.0),
                    "message": dg.MetadataValue.text("ë‹¤ìš´ë¡œë“œí•  PNG íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                }
            )
        
        context.log.info(f"ì´ {total_files}ê°œì˜ PNG íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")

        start_time = time.time()
        downloaded_count = 0

        for obj in all_png_objects:
            try:
                response = s3_client.get_object(Bucket=R2_SOURCE_BUCKET, Key=obj["Key"])
                png_data = response['Body'].read()
                safe_filename = obj["Key"].replace("/", "_").replace("\\", "_")
                local_file_path = ds_image_paths.source_png_dir / safe_filename
                with open(local_file_path, 'wb') as f:
                    f.write(png_data)
                downloaded_files.append(str(local_file_path))
                
                downloaded_count += 1

                elapsed_time = time.time() - start_time
                avg_time_per_file = elapsed_time / downloaded_count if downloaded_count > 0 else 0
                remaining_files = total_files - downloaded_count
                estimated_time_remaining_seconds = avg_time_per_file * remaining_files
                
                formatted_eta = "ê³„ì‚° ì¤‘..."
                if downloaded_count > 0 and remaining_files > 0 : # Start estimating after first file and if files remain
                    eta_minutes = int(estimated_time_remaining_seconds // 60)
                    eta_seconds = int(estimated_time_remaining_seconds % 60)
                    formatted_eta = f"{eta_minutes}ë¶„ {eta_seconds}ì´ˆ"
                elif remaining_files == 0:
                    formatted_eta = "ê±°ì˜ ì™„ë£Œ"

                context.log.info(
                    f"ì§„í–‰: {downloaded_count}/{total_files} ('{obj['Key']}') | ê²½ê³¼: {elapsed_time:.2f}ì´ˆ | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {formatted_eta}"
                )

            except Exception as e_file:
                context.log.error(f"íŒŒì¼ '{obj['Key']}' ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_file}")
                # Optionally, re-raise or collect failed files if needed
                continue # Continue with the next file

        total_download_time_seconds = time.time() - start_time
        context.log.info(f"ğŸ¯ ì´ {len(downloaded_files)}ê°œì˜ PNG íŒŒì¼ì„ {total_download_time_seconds:.2f}ì´ˆ ë§Œì— ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤. (ë²„í‚· ë‚´ ì´ PNG: {total_files}ê°œ)")

    except Exception as e:
        context.log.error(f"R2 ë²„í‚· '{R2_SOURCE_BUCKET}'ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì£¼ìš” ì˜¤ë¥˜ ë°œìƒ: {e}")
        # Ensure total_download_time_seconds is set if error occurs before loop finishes
        if 'start_time' in locals():
             total_download_time_seconds = time.time() - start_time
        else:
             total_download_time_seconds = 0.0 # Or handle as appropriate
        raise

    return dg.MaterializeResult(
        metadata={
            "dagster/row_count": dg.MetadataValue.int(len(downloaded_files)),
            "source_bucket": dg.MetadataValue.text(R2_SOURCE_BUCKET),
            "downloaded_files_count": dg.MetadataValue.int(len(downloaded_files)),
            "total_files_in_bucket_png": dg.MetadataValue.int(total_files if 'total_files' in locals() else 0),
            "total_download_time_seconds": dg.MetadataValue.float(total_download_time_seconds),
            "local_storage_dir": dg.MetadataValue.text(str(ds_image_paths.source_png_dir)),
            "preview_files": dg.MetadataValue.json([Path(f).name for f in downloaded_files[:10]]),
            "processing_timestamp": dg.MetadataValue.text(datetime.now().isoformat())
        }
    )


@dg.asset(
    name="ds_png2webp",
    group_name="DS",
    kinds={"compute"},
    tags={
        "domain": "image_processing",
        "data_tier": "silver",
        "source": "internal"
    },
    deps=["ds_r2_download"],
    description="DS í”„ë¡œì íŠ¸ - ë¡œì»¬ PNG íŒŒì¼ì„ WebP í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ë° ì €ì¥ (Silver Tier)"
)
def ds_png2webp(context: dg.AssetExecutionContext) -> dg.MaterializeResult:
    """
    ë¡œì»¬ PNG ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ì„ ì½ì–´ WebP í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    ë³€í™˜ëœ WebP íŒŒì¼ ê²½ë¡œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    ds_image_paths.ensure_directories()
    png_files = list(ds_image_paths.source_png_dir.glob("*.png"))
    if not png_files:
        context.log.info("ë³€í™˜í•  PNG íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return dg.MaterializeResult(
            metadata={
                "dagster/row_count": dg.MetadataValue.int(0),
                "converted_files_count": dg.MetadataValue.int(0),
                "message": dg.MetadataValue.text("ë³€í™˜í•  PNG íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            }
        )
    # ê¸°ì¡´ WebP íŒŒì¼ ì •ë¦¬
    for existing_file in ds_image_paths.converted_webp_dir.glob("*.webp"):
        existing_file.unlink()
    converted_files = []
    for png_path in png_files:
        try:
            with Image.open(png_path) as img:
                webp_filename = png_path.stem + ".webp"
                webp_path = ds_image_paths.converted_webp_dir / webp_filename
                img.save(webp_path, format="WEBP", quality=80, optimize=True)
                converted_files.append(str(webp_path))
                context.log.info(f"âœ“ ë³€í™˜ ì™„ë£Œ: '{png_path.name}' â†’ '{webp_filename}'")
        except Exception as e:
            context.log.error(f"âŒ íŒŒì¼ '{png_path.name}' ë³€í™˜ ì‹¤íŒ¨: {e}")
            continue
    return dg.MaterializeResult(
        metadata={
            "dagster/row_count": dg.MetadataValue.int(len(converted_files)),
            "converted_files_count": dg.MetadataValue.int(len(converted_files)),
            "preview_converted_files": dg.MetadataValue.json([Path(f).name for f in converted_files[:10]]),
            "processing_timestamp": dg.MetadataValue.text(datetime.now().isoformat())
        }
    )


@dg.asset(
    name="ds_img_finalize",
    group_name="DS",
    kinds={"compute"},
    tags={
        "domain": "image_processing",
        "data_tier": "silver",
        "source": "internal"
    },
    deps=["ds_png2webp"],
    description="DS í”„ë¡œì íŠ¸ - PNG íŒŒì¼ì„ ìµœì¢… ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ê³  company.logo ì»¬ëŸ¼ì„ public URLë¡œ ì—…ë°ì´íŠ¸, DuckDB ê¸°ë¡ (webpë„ í•¨ê»˜ ê¸°ë¡)"
)
def ds_img_finalize(context: dg.AssetExecutionContext, ds_duckdb: DuckDBResource, ds_postgres: PostgresResource) -> dg.MaterializeResult: 
    """
    PNG íŒŒì¼ì„ ìµœì¢… ë””ë ‰í† ë¦¬ë¡œ ì´ë™, webpë„ í•¨ê»˜ ì´ë™. company.logo ì»¬ëŸ¼ì€ png public urlë¡œë§Œ ì—…ë°ì´íŠ¸. DuckDBì—ëŠ” png/webp ëª¨ë‘ ê¸°ë¡.
    """
    if not DS_PUBLIC_URL:
        raise ValueError("DS_PUBLIC_URLì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    ds_image_paths.ensure_directories()
    final_dir = ds_image_paths.base_dir / "final"
    final_dir.mkdir(parents=True, exist_ok=True)
    png_files = list(ds_image_paths.source_png_dir.glob("*.png"))
    if not png_files:
        context.log.info("ì´ë™í•  PNG íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return dg.MaterializeResult(
            metadata={
                "dagster/row_count": dg.MetadataValue.int(0),
                "updated_companies": dg.MetadataValue.int(0),
                "message": dg.MetadataValue.text("ì´ë™í•  PNG íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            }
        )
    # postgres_config = {
    #     "host": POSTGRES_HOST,
    #     "port": int(POSTGRES_PORT) if POSTGRES_PORT else 5432,
    #     "user": POSTGRES_USER,
    #     "password": POSTGRES_PASSWORD,
    #     "dbname": POSTGRES_DB
    # }
    updated_companies = []
    failed_updates = []
    context.log.info(f"{len(png_files)}ê°œ PNG íŒŒì¼ ì´ë™ ë° company.logo ì—…ë°ì´íŠ¸ ì‹œì‘ (webpë„ í•¨ê»˜ ê¸°ë¡)")
    try:
        with ds_postgres.get_connection() as pg_conn:
            with pg_conn.cursor(cursor_factory=RealDictCursor) as cur:
                for png_path in png_files:
                    try:
                        company_id = extract_cid(png_path.name)
                        # PNG íŒŒì¼ ì´ë™
                        final_png_path = final_dir / png_path.name
                        png_path.rename(final_png_path)
                        # webp íŒŒì¼ë„ ìˆìœ¼ë©´ ê°™ì´ ì´ë™
                        webp_path =ds_image_paths.converted_webp_dir / (png_path.stem + ".webp")
                        final_webp_path = None
                        if webp_path.exists():
                            final_webp_path = final_dir / webp_path.name
                            webp_path.rename(final_webp_path)
                        # logo_urlì€ pngë§Œ ì‚¬ìš©
                        logo_url = f"{DS_PUBLIC_URL.rstrip('/')}/{final_png_path.name}"
                        cur.execute(
                            "SELECT id, name FROM company WHERE id = %s",
                            (company_id,)
                        )
                        company_record = cur.fetchone()
                        # --- R2 íƒ€ê²Ÿ ë²„í‚· ì—…ë¡œë“œ ---
                        s3_client = get_r2_client()
                        # PNG ì—…ë¡œë“œ
                        try:
                            s3_client.upload_file(str(final_png_path), R2_TARGET_BUCKET, final_png_path.name)
                            context.log.info(f"âœ“ PNG ì—…ë¡œë“œ: {final_png_path.name} â†’ {R2_TARGET_BUCKET}")
                        except Exception as e:
                            context.log.error(f"âŒ PNG ì—…ë¡œë“œ ì‹¤íŒ¨: {final_png_path.name} - {e}")
                        # WebP ì—…ë¡œë“œ
                        if final_webp_path:
                            try:
                                s3_client.upload_file(str(final_webp_path), R2_TARGET_BUCKET, final_webp_path.name)
                                context.log.info(f"âœ“ WebP ì—…ë¡œë“œ: {final_webp_path.name} â†’ {R2_TARGET_BUCKET}")
                            except Exception as e:
                                context.log.error(f"âŒ WebP ì—…ë¡œë“œ ì‹¤íŒ¨: {final_webp_path.name} - {e}")
                        # --- ê¸°ì¡´ DB ì²˜ë¦¬ ê³„ì† ---
                        if company_record:
                            cur.execute(
                                "UPDATE company SET thumbnail = %s WHERE id = %s",
                                (logo_url, company_id)
                            )
                            if cur.rowcount > 0:
                                updated_companies.append({
                                    "id": company_id,
                                    "name": company_record["name"],
                                    "thumbnail": logo_url,
                                    "filename_png": final_png_path.name,
                                    "filename_webp": final_webp_path.name if final_webp_path else None,
                                    "updated_at": datetime.now().isoformat()
                                })
                                context.log.info(f"âœ“ íšŒì‚¬ ë¡œê³ (png) ì—…ë°ì´íŠ¸ ì™„ë£Œ: '{company_record['name']}' (ID: {company_id}) â†’ {logo_url}")
                            else:
                                failed_updates.append({
                                    "id": id,
                                    "filename": final_png_path.name,
                                    "error": "UPDATE ì¿¼ë¦¬ê°€ 0ê°œ í–‰ì— ì˜í–¥ì„ ì¤Œ"
                                })
                                context.log.error(f"âŒ FAST-FAIL: UPDATE ì‹¤íŒ¨ - id '{id}' (íŒŒì¼: {final_png_path.name})")
                                raise Exception(f"UPDATE ì‹¤íŒ¨ - id '{id}'ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ê°€ 0ê°œ í–‰ì— ì˜í–¥ì„ ì¤Œ")
                        else:
                            failed_updates.append({
                                "id": id,
                                "filename": final_png_path.name,
                                "error": f"PostgreSQLì—ì„œ id '{id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
                            })
                            context.log.error(f"âŒ FAST-FAIL: íšŒì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - id '{id}' (íŒŒì¼: {final_png_path.name})")
                            raise Exception(f"PostgreSQLì—ì„œ id '{id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    except Exception as e:
                        context.log.error(f"âŒ FAST-FAIL: íŒŒì¼ '{png_path.name}' ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
                        failed_updates.append({
                            "id": id if 'id' in locals() else 'unknown',
                            "filename": png_path.name,
                            "error": str(e)
                        })
                        raise
                pg_conn.commit()
    except Exception as e:
        context.log.error(f"PostgreSQL ì—°ê²° ë˜ëŠ” ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise
    try:
        with ds_duckdb.get_connection() as duck_conn:
            duck_conn.execute("""
                CREATE TABLE IF NOT EXISTS ds_logo_updates (
                    id VARCHAR,
                    name VARCHAR,
                    thumbnail VARCHAR,
                    filename_png VARCHAR,
                    filename_webp VARCHAR,
                    updated_at TIMESTAMP,
                    processing_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            if updated_companies:
                duck_conn.executemany(
                    """
                    INSERT INTO ds_logo_updates (
                        id, name, thumbnail, filename_png, filename_webp, updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?)
                    """,
                    [
                        (
                            record["id"],
                            record["name"],
                            record["thumbnail"],
                            record["filename_png"],
                            record["filename_webp"],
                            record["updated_at"]
                        )
                        for record in updated_companies
                    ]
                )
                context.log.info(f"ğŸ“ DuckDBì— {len(updated_companies)}ê°œ ì—…ë°ì´íŠ¸ ê¸°ë¡ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        context.log.error(f"DuckDB ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨: {e}")
    context.log.info(
        f"ğŸ¯ ë¡œê³  ì—…ë°ì´íŠ¸ ì™„ë£Œ ìš”ì•½: {len(updated_companies)}ê°œ ì„±ê³µ, {len(failed_updates)}ê°œ ì‹¤íŒ¨"
    )
    return dg.MaterializeResult(
        metadata={
            "dagster/row_count": dg.MetadataValue.int(len(updated_companies)),
            "updated_companies": dg.MetadataValue.int(len(updated_companies)),
            "failed_updates": dg.MetadataValue.int(len(failed_updates)),
            "ds_public_url": dg.MetadataValue.text(DS_PUBLIC_URL),
            "processed_files": dg.MetadataValue.int(len(png_files)),
            "preview_updated_companies": dg.MetadataValue.json(
                [f"{c['name']} ({c['id']})" for c in updated_companies[:5]]
            ),
            "processing_timestamp": dg.MetadataValue.text(datetime.now().isoformat())
        }
    )


@dg.asset(
    group_name="DS",
    kinds={"cleanup"},
    tags={
        "domain": "image_processing",
        "data_tier": "utility",
        "source": "cloudflare_r2"
    },
    description="DS í”„ë¡œì íŠ¸ - R2 ì†ŒìŠ¤ ë²„í‚·ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì‚­ì œ (ì˜ì¡´ì„± ì—†ìŒ, ìœ„í—˜: ë˜ëŒë¦´ ìˆ˜ ì—†ìŒ)"
)
def ds_r2_clear_src(context: dg.AssetExecutionContext) -> dg.MaterializeResult:
    """
    R2 ì†ŒìŠ¤ ë²„í‚·ì˜ ëª¨ë“  PNG, WEBP ì´ë¯¸ì§€ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    if not R2_SOURCE_BUCKET:
        raise ValueError("R2_SOURCE_BUCKETì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    s3_client = get_r2_client()
    deleted = []
    errors = []
    try:
        context.log.info(f"R2 ì†ŒìŠ¤ ë²„í‚· '{R2_SOURCE_BUCKET}'ì—ì„œ ì´ë¯¸ì§€ ì‚­ì œ ì‹œì‘")
        paginator = s3_client.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=R2_SOURCE_BUCKET):
            if "Contents" in page:
                for obj in page["Contents"]:
                    key = obj["Key"]
                    if key.lower().endswith(('.png', '.webp')):
                        try:
                            s3_client.delete_object(Bucket=R2_SOURCE_BUCKET, Key=key)
                            deleted.append(key)
                            context.log.info(f"âœ“ ì‚­ì œ: {key}")
                        except Exception as e:
                            errors.append({"key": key, "error": str(e)})
                            context.log.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {key} - {e}")
        context.log.info(f"ğŸ¯ ì´ {len(deleted)}ê°œ ì´ë¯¸ì§€ ì‚­ì œ ì™„ë£Œ, ì‹¤íŒ¨ {len(errors)}ê°œ")
    except Exception as e:
        context.log.error(f"R2 ë²„í‚· '{R2_SOURCE_BUCKET}' ì´ë¯¸ì§€ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise
    return dg.MaterializeResult(
        metadata={
            "dagster/row_count": dg.MetadataValue.int(len(deleted)),
            "deleted_files": dg.MetadataValue.json(deleted[:10]),
            "error_count": dg.MetadataValue.int(len(errors)),
            "errors": dg.MetadataValue.json(errors[:5]),
            "bucket": dg.MetadataValue.text(R2_SOURCE_BUCKET),
            "processing_timestamp": dg.MetadataValue.text(datetime.now().isoformat())
        }
    )


@dg.asset(
    name="ds_r2_clear_tgt",
    group_name="DS",
    kinds={"cleanup"},
    tags={
        "domain": "image_processing",
        "data_tier": "utility",
        "source": "cloudflare_r2"
    },
    description="DS í”„ë¡œì íŠ¸ - R2 íƒ€ê²Ÿ ë²„í‚·ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì‚­ì œ (ì˜ì¡´ì„± ì—†ìŒ, ìœ„í—˜: ë˜ëŒë¦´ ìˆ˜ ì—†ìŒ)"
)
def ds_r2_clear_tgt(context: dg.AssetExecutionContext) -> dg.MaterializeResult:
    """
    R2 íƒ€ê²Ÿ ë²„í‚·ì˜ ëª¨ë“  PNG, WEBP ì´ë¯¸ì§€ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    if not R2_TARGET_BUCKET:
        raise ValueError("R2_TARGET_BUCKETì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    s3_client = get_r2_client()
    deleted = []
    errors = []
    try:
        context.log.info(f"R2 íƒ€ê²Ÿ ë²„í‚· '{R2_TARGET_BUCKET}'ì—ì„œ ì´ë¯¸ì§€ ì‚­ì œ ì‹œì‘")
        paginator = s3_client.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=R2_TARGET_BUCKET):
            if "Contents" in page:
                for obj in page["Contents"]:
                    key = obj["Key"]
                    if key.lower().endswith(('.png', '.webp')):
                        try:
                            s3_client.delete_object(Bucket=R2_TARGET_BUCKET, Key=key)
                            deleted.append(key)
                            context.log.info(f"âœ“ ì‚­ì œ: {key}")
                        except Exception as e:
                            errors.append({"key": key, "error": str(e)})
                            context.log.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {key} - {e}")
        context.log.info(f"ğŸ¯ ì´ {len(deleted)}ê°œ ì´ë¯¸ì§€ ì‚­ì œ ì™„ë£Œ, ì‹¤íŒ¨ {len(errors)}ê°œ")
    except Exception as e:
        context.log.error(f"R2 ë²„í‚· '{R2_TARGET_BUCKET}' ì´ë¯¸ì§€ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise
    return dg.MaterializeResult(
        metadata={
            "dagster/row_count": dg.MetadataValue.int(len(deleted)),
            "deleted_files": dg.MetadataValue.json(deleted[:10]),
            "error_count": dg.MetadataValue.int(len(errors)),
            "errors": dg.MetadataValue.json(errors[:5]),
            "bucket": dg.MetadataValue.text(R2_TARGET_BUCKET),
            "processing_timestamp": dg.MetadataValue.text(datetime.now().isoformat())
        }
    )


@dg.asset_check(
    asset="ds_r2_download",
)
def ds_r2_src_check(context: dg.AssetCheckExecutionContext):
    """R2 ì†ŒìŠ¤ ë²„í‚· ì—°ê²° ìƒíƒœ ê²€ì¦"""
    try:
        s3_client = get_r2_client()
        s3_client.head_bucket(Bucket=R2_SOURCE_BUCKET)
        return dg.AssetCheckResult(
            passed=True,
            metadata={"bucket_name": dg.MetadataValue.text(R2_SOURCE_BUCKET)}
        )
    except Exception as e:
        return dg.AssetCheckResult(
            passed=False,
            metadata={
                "bucket_name": dg.MetadataValue.text(R2_SOURCE_BUCKET),
                "error": dg.MetadataValue.text(str(e))
            }
        )


@dg.asset_check(
    asset="ds_upload_webp_to_r2",
)
def ds_r2_tgt_check(context: dg.AssetCheckExecutionContext):
    """R2 íƒ€ê²Ÿ ë²„í‚· ì—°ê²° ìƒíƒœ ê²€ì¦"""
    try:
        s3_client = get_r2_client()
        s3_client.head_bucket(Bucket=R2_TARGET_BUCKET)
        return dg.AssetCheckResult(
            passed=True,
            metadata={"bucket_name": dg.MetadataValue.text(R2_TARGET_BUCKET)}
        )
    except Exception as e:
        return dg.AssetCheckResult(
            passed=False,
            metadata={
                "bucket_name": dg.MetadataValue.text(R2_TARGET_BUCKET),
                "error": dg.MetadataValue.text(str(e))
            }
        )


def extract_cid(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ company_idë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì˜ˆ: clogo-0038de98-fbe5-4ce4-a139-0b2e10ac028f.webp â†’ 0038de98-fbe5-4ce4-a139-0b2e10ac028f
    """
    # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
    name_without_ext = Path(filename).stem
    
    # clogo- ì ‘ë‘ì‚¬ ì œê±°í•˜ê³  company_id ì¶”ì¶œ
    if name_without_ext.startswith("clogo-"):
        return name_without_ext[6:]  # "clogo-" ì œê±°
    else:
        # ë§Œì•½ ë‹¤ë¥¸ íŒ¨í„´ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return name_without_ext

