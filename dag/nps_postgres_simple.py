"""
NPS PostgreSQL ê°„ì†Œí™” ë²„ì „
í•µì‹¬ ì›ì¹™:
1. PostgreSQL COPY ìµœì í™”: StringIO ìŠ¤íŠ¸ë¦¬ë°
2. ì‹œê·¸ë„ í•¸ë“¤ë§: ê°•ì œ ì¢…ë£Œ ì‹œ ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
"""

import dagster as dg
import pandas as pd
import signal
import threading
import time
import io
import re
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
from .resources import PostgresResource

# ğŸ”¥ í•µì‹¬ ì„¤ì •ë§Œ ìœ ì§€
POSTGRES_NPS_TABLE = "pension"
DEFAULT_MAX_WORKERS = 2

# ğŸš¨ ì¸ë±ìŠ¤ ë° ì œì•½ì¡°ê±´ ê´€ë¦¬ (ëŒ€ëŸ‰ ì‚½ì… ìµœì í™”)
# ì‚½ì… ì „ ì œê±°í•  ì¸ë±ìŠ¤ë“¤
DROP_INDEXES_SQL = [
    "DROP INDEX IF EXISTS pension_opt_date_idx",
    "DROP INDEX IF EXISTS pension_opt_company_name_idx", 
    "DROP INDEX IF EXISTS pension_opt_industry_code_idx",
    "DROP INDEX IF EXISTS pension_opt_zip_code_idx",
    "DROP INDEX IF EXISTS pension_opt_region_industry_idx",
    "DROP INDEX IF EXISTS pension_opt_unique_business_month"
]

# ì»¬ëŸ¼ ëª©ë¡ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì •ì˜í•˜ì—¬ ì¬ì‚¬ìš©
# ì´ ìˆœì„œëŠ” PostgreSQL í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ìˆœì„œ ë° COPY ëª…ë ¹ì–´ì˜ ì»¬ëŸ¼ ìˆœì„œì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
CSV_COLUMNS_SCHEMA = [
    'data_created_ym',         # ìë£Œìƒì„±ë…„ì›”
    'company_name',            # ì‚¬ì—…ì¥ëª…
    'business_reg_num',        # ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸
    'join_status',             # ì‚¬ì—…ì¥ê°€ì…ìƒíƒœì½”ë“œ
    'zip_code',                # ìš°í¸ë²ˆí˜¸
    'lot_number_address',      # ì‚¬ì—…ì¥ì§€ë²ˆìƒì„¸ì£¼ì†Œ
    'road_name_address',       # ì‚¬ì—…ì¥ë„ë¡œëª…ìƒì„¸ì£¼ì†Œ
    'legal_dong_addr_code',    # ê³ ê°ë²•ì •ë™ì£¼ì†Œì½”ë“œ
    'admin_dong_addr_code',    # ê³ ê°í–‰ì •ë™ì£¼ì†Œì½”ë“œ
    'addr_sido_code',          # ë²•ì •ë™ì£¼ì†Œê´‘ì—­ì‹œë„ì½”ë“œ
    'addr_sigungu_code',       # ì‹œêµ°êµ¬ì½”ë“œ
    'addr_emdong_code',        # ìë©´ë™ì½”ë“œ
    'workplace_type',          # ë²•ì¸/ê°œì¸
    'industry_code',           # ì—…ì¢…ì½”ë“œ
    'industry_name',           # ì—…ì¢…ëª…
    'applied_at',              # ì ìš©ì¼ì
    're_registered_at',        # ì¬ë“±ë¡ì¼ì
    'withdrawn_at',            # íƒˆí‡´ì¼ì
    'subscriber_count',        # ê°€ì…ììˆ˜
    'monthly_notice_amount',   # ë‹¹ì›”ê³ ì§€ê¸ˆì•¡
    'new_subscribers',         # ì‹ ê·œì·¨ë“ììˆ˜
    'lost_subscribers'         # ìƒì‹¤ê°€ì…ììˆ˜
]


def drop_indexes_and_constraints(postgres: PostgresResource, context=None) -> bool:
    """ëŒ€ëŸ‰ ì‚½ì… ì „ ì¸ë±ìŠ¤ì™€ ì œì•½ì¡°ê±´ ì œê±°"""
    try:
        with postgres.get_connection() as conn:
            with conn.cursor() as cursor:
                if context:
                    context.log.info("ğŸ—‘ï¸ ì¸ë±ìŠ¤ ì œê±° ì‹œì‘ (ì‚½ì… ì„±ëŠ¥ ìµœì í™”)")
                else:
                    print("ğŸ—‘ï¸ ì¸ë±ìŠ¤ ì œê±° ì‹œì‘ (ì‚½ì… ì„±ëŠ¥ ìµœì í™”)")
                
                for drop_sql in DROP_INDEXES_SQL:
                    try:
                        cursor.execute(drop_sql)
                        if context:
                            context.log.info(f"âœ… {drop_sql}")
                        else:
                            print(f"âœ… {drop_sql}")
                    except Exception as e:
                        # ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ” ë¬´ì‹œ
                        if context:
                            context.log.warning(f"âš ï¸ {drop_sql} - {str(e)}")
                        else:
                            print(f"âš ï¸ {drop_sql} - {str(e)}")
                
                conn.commit()
                
                if context:
                    context.log.info("ğŸ ì¸ë±ìŠ¤ ì œê±° ì™„ë£Œ")
                else:
                    print("ğŸ ì¸ë±ìŠ¤ ì œê±° ì™„ë£Œ")
                return True
                
    except Exception as e:
        if context:
            context.log.error(f"ğŸ’¥ ì¸ë±ìŠ¤ ì œê±° ì‹¤íŒ¨: {str(e)}")
        else:
            print(f"ğŸ’¥ ì¸ë±ìŠ¤ ì œê±° ì‹¤íŒ¨: {str(e)}")
        return False


# ğŸš¨ ì‹œê·¸ë„ í•¸ë“¤ë§ ë° Semaphore ê¸°ë°˜ ì›Œì»¤ ìŠ¬ë¡¯ ì œì–´
_shutdown_requested = threading.Event()
_active_executors = set()
_executor_lock = threading.Lock()
_worker_semaphore = None  # ë™ì ìœ¼ë¡œ ì„¤ì •ë¨
_worker_stats = {}  # ì›Œì»¤ë³„ í†µê³„ ì •ë³´
_stats_lock = threading.Lock()

def signal_handler(signum, frame):
    """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ - ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    print(f"\nğŸš¨ ì‹œê·¸ë„ {signum} ê°ì§€ - ì•ˆì „í•œ ì¢…ë£Œ ì‹œì‘")
    _shutdown_requested.set()
    
    # í™œì„± executor ì •ë¦¬
    with _executor_lock:
        for executor in list(_active_executors):
            try:
                executor.shutdown(wait=False)
            except Exception:
                pass

def install_signal_handlers():
    """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì¹˜"""
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    print("ğŸ›¡ï¸ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì¹˜ ì™„ë£Œ")

def init_worker_semaphore(max_workers: int):
    """ì›Œì»¤ Semaphore ì´ˆê¸°í™”"""
    global _worker_semaphore, _worker_stats
    _worker_semaphore = threading.Semaphore(max_workers)
    _worker_stats.clear()
    print(f"ğŸ¯ ì›Œì»¤ Semaphore ì´ˆê¸°í™”: {max_workers}ê°œ ìŠ¬ë¡¯")

def get_worker_stats() -> Dict:
    """ì›Œì»¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
    with _stats_lock:
        return _worker_stats.copy()

def update_worker_stats(worker_id: str, file_path: str, records: int, duration: float, success: bool):
    """ì›Œì»¤ í†µê³„ ì—…ë°ì´íŠ¸"""
    with _stats_lock:
        if worker_id not in _worker_stats:
            _worker_stats[worker_id] = {
                'files_processed': 0,
                'total_records': 0,
                'total_duration': 0.0,
                'successful_files': 0,
                'failed_files': 0,
                'avg_records_per_second': 0.0
            }
        
        stats = _worker_stats[worker_id]
        stats['files_processed'] += 1
        stats['total_records'] += records
        stats['total_duration'] += duration
        
        if success:
            stats['successful_files'] += 1
        else:
            stats['failed_files'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì†ë„ ê³„ì‚°
        if stats['total_duration'] > 0:
            stats['avg_records_per_second'] = stats['total_records'] / stats['total_duration']

def register_executor(executor):
    """í™œì„± executor ë“±ë¡"""
    with _executor_lock:
        _active_executors.add(executor)

def unregister_executor(executor):
    """executor ë“±ë¡ í•´ì œ"""
    with _executor_lock:
        _active_executors.discard(executor)

# NPSFilePaths ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
@dataclass
class NPSFilePaths:
    """NPS íŒŒì¼ ê²½ë¡œ ê´€ë¦¬"""
    base_dir: Path = Path("data") / "nps" 
    
    @property
    def out_history_dir(self) -> Path:
        return self.base_dir / "history" / "out"
        
    @property
    def processed_history_dir(self) -> Path:
        return self.base_dir / "history" / "processed_history"

PATHS = NPSFilePaths()

class SimpleFileResult:
    """ê°„ë‹¨í•œ íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼"""
    def __init__(self, file_path: str, success: bool, records: int = 0, error: str = None):
        self.file_path = file_path
        self.success = success
        self.records = records
        self.error = error

def get_file_date(filename: str) -> Optional[str]:
    """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ê°„ì†Œí™”)"""
    import re
    
    # YYYYë…„ MMì›” íŒ¨í„´
    match = re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”', filename)
    if match:
        year, month = match.groups()
        return f"{year}-{month.zfill(2)}"
    
    # YYYYMM íŒ¨í„´
    match = re.search(r'(\d{4})(\d{2})', filename)
    if match:
        year, month = match.groups()
        return f"{year}-{month}"
    
    return None

def convert_to_copy_format(df: pd.DataFrame, ordered_columns: List[str]) -> str:
    """
    DataFrameì„ PostgreSQL COPY í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ìµœì í™”ë¨)
    StringIO ìŠ¤íŠ¸ë¦¬ë° ë° df.to_csv() ì‚¬ìš©.
    process_single_fileì—ì„œ ëª¨ë“  ë°ì´í„° í´ë¦¬ë‹ ë° NA ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    """
    output = io.StringIO()
    # DataFrameì˜ ì»¬ëŸ¼ ìˆœì„œë¥¼ ordered_columnsì— ë§ì¶°ì„œ CSVë¡œ ë³€í™˜
    # process_single_fileì—ì„œ NA ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ì¶”ê°€ì ì¸ replace ë¶ˆí•„ìš”
    df.to_csv(output, sep='\t', header=False, index=False, na_rep='\\N', columns=ordered_columns)
    return output.getvalue()

def process_single_file(csv_file: Path, data_created_ym: str, postgres: PostgresResource, context: dg.AssetExecutionContext) -> SimpleFileResult:
    """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (ì¤‘ë³µ ì œê±° í¬í•¨, Pandas ì—°ì‚° ìµœì í™”)"""
    try:
        if _shutdown_requested.is_set():
            return SimpleFileResult(str(csv_file), False, 0, "Shutdown requested")

        # CSV ì½ê¸° (ìŠ¤í‚¤ë§ˆì— ì •ì˜ëœ ì»¬ëŸ¼ëª… ì‚¬ìš©)
        df = pd.read_csv(csv_file, header=None, skiprows=1, dtype=str, encoding='utf-8')
        df.columns = CSV_COLUMNS_SCHEMA # ì „ì—­ ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©

        # ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•´ í´ë¦¬ë‹ ë° NA ì²˜ë¦¬ (data_created_ymëŠ” ë³„ë„ ì²˜ë¦¬)
        for col_name in df.columns:
            if col_name == 'data_created_ym':
                continue

            df[col_name] = df[col_name].fillna('').astype(str)
            df[col_name] = df[col_name].str.replace('\t', ' ', regex=False)
            df[col_name] = df[col_name].str.replace('\n', ' ', regex=False)
            df[col_name] = df[col_name].str.replace('\r', ' ', regex=False)
            df[col_name] = df[col_name].str.strip()
            df[col_name] = df[col_name].replace('', pd.NA)
            
        df['data_created_ym'] = f"{data_created_ym}-15 00:00:00"
        
        unique_key = [
            'data_created_ym', 'company_name', 'business_reg_num', 'zip_code', 
            'subscriber_count', 'monthly_notice_amount'
        ]
        
        before_dedup = len(df)
        df.drop_duplicates(subset=unique_key, inplace=True, keep='first')
        after_dedup = len(df)

        deduplicated_rows = before_dedup - after_dedup
        if deduplicated_rows > 0:
            context.log.info(f"{csv_file.name} - {deduplicated_rows} duplicate rows removed based on unique key constraint.")

        total_records = len(df)
        if total_records == 0:
            context.log.info(f"{csv_file.name} - No records to process after deduplication.")
            return SimpleFileResult(str(csv_file), True, 0)
        
        copy_data = convert_to_copy_format(df, CSV_COLUMNS_SCHEMA)
        
        with postgres.get_connection() as conn:
            with conn.cursor() as cursor:
                copy_stream = io.StringIO(copy_data)
                try:
                    cursor.copy_from(
                        copy_stream,
                        POSTGRES_NPS_TABLE,
                        columns=CSV_COLUMNS_SCHEMA,
                        sep='\t',
                        null='\\N'
                    )
                    conn.commit()
                except Exception as e:
                    conn.rollback()
                    context_data_for_log = copy_data[:500] 
                    context.log.error(f"COPY command failed for {csv_file.name}. Data preview: {context_data_for_log}")
                    raise e
        
        return SimpleFileResult(str(csv_file), True, total_records)
        
    except Exception as e:
        return SimpleFileResult(str(csv_file), False, 0, str(e))

def process_single_file_with_semaphore(csv_file: Path, data_created_ym: str, postgres: PostgresResource, context: dg.AssetExecutionContext) -> SimpleFileResult:
    """Semaphore ê¸°ë°˜ ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (ì •í™•í•œ ì›Œì»¤ë³„ ì‹œê°„ ì¸¡ì •)"""
    worker_id = threading.current_thread().name
    start_time = time.time()
    
    _worker_semaphore.acquire()
    try:
        context.log.info(f"ğŸ”· [{worker_id}] ìŠ¬ë¡¯ íšë“ - {csv_file.name} ì²˜ë¦¬ ì‹œì‘")
        
        result = process_single_file(csv_file, data_created_ym, postgres, context)
        
        duration = time.time() - start_time
        
        update_worker_stats(worker_id, str(csv_file), result.records, duration, result.success)
        
        if result.success:
            context.log.info(f"âœ… [{worker_id}] {csv_file.name}: {result.records:,}ê±´ ì™„ë£Œ ({duration:.2f}ì´ˆ, {result.records/duration:.0f}ê±´/ì´ˆ)")
        else:
            context.log.error(f"âŒ [{worker_id}] {csv_file.name}: ì‹¤íŒ¨ - {result.error} ({duration:.2f}ì´ˆ)")
        
        return result
        
    finally:
        _worker_semaphore.release()
        context.log.info(f"ğŸ”¶ [{worker_id}] ìŠ¬ë¡¯ í•´ì œ - {csv_file.name} ì²˜ë¦¬ ì™„ë£Œ")

def process_files_parallel(file_info: Dict[Path, str], postgres: PostgresResource, max_workers: int, context: dg.AssetExecutionContext) -> Dict:
    """ë³‘ë ¬ íŒŒì¼ ì²˜ë¦¬ (Semaphore ê¸°ë°˜ ì›Œì»¤ ìŠ¬ë¡¯ ì œì–´ + Fast-Fail ì ìš©)"""
    install_signal_handlers() # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ëŠ” printë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ë³„ë„ ë¡œê±° ì‚¬ìš© ê³ ë ¤
    
    file_info = dict(sorted(file_info.items(), key=lambda x: x[0].stat().st_size, reverse=True))

    init_worker_semaphore(max_workers) # Semaphore ì´ˆê¸°í™” ë©”ì‹œì§€ëŠ” print ìœ ì§€ ë˜ëŠ” ë³„ë„ ë¡œê±°
    
    results = []
    successful_files = 0
    failed_files = 0
    total_records = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        register_executor(executor)
        
        try:
            future_to_file = {
                executor.submit(process_single_file_with_semaphore, csv_file, date_str, postgres, context): csv_file
                for csv_file, date_str in file_info.items()
            }

            for future in as_completed(future_to_file):
                if _shutdown_requested.is_set():
                    context.log.warning("ğŸš¨ ì¢…ë£Œ ì‹ í˜¸ ê°ì§€ - ì²˜ë¦¬ ì¤‘ë‹¨")
                    break

                try:
                    csv_file = future_to_file[future]
                    result = future.result()
                    results.append(result)
                    
                    if result.success:
                        successful_files += 1
                        total_records += result.records
                    else:
                        failed_files += 1
                        context.log.error(f"âŒ {csv_file.name}: ì‹¤íŒ¨ - {result.error}")
                        
                        context.log.error(f"ğŸ’¥ Fast-Fail ë°œë™: ì²« ë²ˆì§¸ íŒŒì¼ ì‹¤íŒ¨ë¡œ ì „ì²´ ì²˜ë¦¬ ì¤‘ë‹¨")
                        _shutdown_requested.set()
                        break
                        
                except Exception as e:
                    failed_files += 1
                    csv_file = future_to_file[future]
                    context.log.error(f"âŒ {csv_file.name}: ì˜ˆì™¸ ë°œìƒ - {str(e)}")
                    
                    context.log.error(f"ğŸ’¥ Fast-Fail ë°œë™: ì˜ˆì™¸ ë°œìƒìœ¼ë¡œ ì „ì²´ ì²˜ë¦¬ ì¤‘ë‹¨")
                    _shutdown_requested.set()
                    break
                    
        finally:
            unregister_executor(executor)
            
            # ì›Œì»¤ë³„ í†µê³„ëŠ” nps_to_postgres_simpleì—ì„œ context.logë¡œ ì¶œë ¥í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¤‘ë³µ ì¶œë ¥ ì•ˆ í•¨
            # í•„ìš”ì‹œ context.log.infoë¡œ ì¶”ê°€ ë¡œê¹… ê°€ëŠ¥
    
    return {
        'successful_files': successful_files,
        'failed_files': failed_files,
        'total_records': total_records,
        'results': results,
        'fast_fail_triggered': failed_files > 0,
        'fast_fail_reason': f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {failed_files}ê°œ íŒŒì¼" if failed_files > 0 else None,
        'worker_stats': get_worker_stats()
    }

def scan_processed_files() -> Dict[Path, str]:
    """ì²˜ë¦¬ëœ íŒŒì¼ ìŠ¤ìº”"""
    processed_files = {}
    
    if not PATHS.out_history_dir.exists():
        return processed_files
    
    for csv_file in PATHS.out_history_dir.glob("*.csv"):
        if csv_file.is_file() and csv_file.stat().st_size > 0:
            date_str = get_file_date(csv_file.name)
            if date_str:
                processed_files[csv_file] = date_str
    
    return processed_files

# ğŸš¨ ë©”ì¸ Asset (ê°„ì†Œí™”)
@dg.asset(
    group_name="NPS",
    description="NPS ë°ì´í„°ë¥¼ PostgreSQLì— ê°„ì†Œí™”ëœ ë°©ì‹ìœ¼ë¡œ ì ì¬",
    # deps=["nps_data_processing"], # ì´ì „ ë‹¨ê³„ê°€ ìˆë‹¤ë©´ ì£¼ì„ í•´ì œ
    metadata={
        "table_name": dg.MetadataValue.text(POSTGRES_NPS_TABLE),
        "processing_method": dg.MetadataValue.text("Optimized Parallel Processing with df.to_csv"),
        "indexes_recreated": dg.MetadataValue.bool(False), # TODO: ë°ì´í„° ì ì¬ í›„ ì¸ë±ìŠ¤ ì¬ìƒì„± ë¡œì§ ì¶”ê°€ í•„ìš”
    },
    tags={"layer": "silver", "database": "postgres", "source": "nps"},
    kinds={"postgres"},
    deps=["nps_data_processing"]
)
def nps_to_postgres_simple(
    context: dg.AssetExecutionContext,
    nps_postgres: PostgresResource,
) -> dg.MaterializeResult:
    """ê°„ì†Œí™”ëœ NPS PostgreSQL ì ì¬"""
    
    max_workers = context.run_config.get("ops", {}).get("nps_to_postgres_simple", {}).get("config", {}).get("max_workers", DEFAULT_MAX_WORKERS)
    
    context.log.info(f"ğŸš€ NPS PostgreSQL ê°„ì†Œí™” ì ì¬ ì‹œì‘ (ì›Œì»¤: {max_workers}ê°œ)")
    
    file_info = scan_processed_files()
    
    if not file_info:
        context.log.warning("âš ï¸ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return dg.MaterializeResult(
            metadata={
                "processed_files": dg.MetadataValue.int(0),
                "total_records": dg.MetadataValue.int(0),
            }
        )
    
    context.log.info(f"ğŸ“ {len(file_info)}ê°œ íŒŒì¼ ë°œê²¬")
    
    context.log.info("ğŸš€ 1ë‹¨ê³„: ì¸ë±ìŠ¤ ì œê±° (ì‚½ì… ì„±ëŠ¥ ìµœì í™”)")
    if not drop_indexes_and_constraints(nps_postgres, context): # drop_indexes_and_constraintsëŠ” ì´ë¯¸ contextë¥¼ ì‚¬ìš©
        context.log.warning("âš ï¸ ì¸ë±ìŠ¤ ì œê±° ì‹¤íŒ¨ - ê³„ì† ì§„í–‰")
    
    try:
        context.log.info("ğŸš€ 2ë‹¨ê³„: ë°ì´í„° ì‚½ì… ì‹œì‘")
        result = process_files_parallel(file_info, nps_postgres, max_workers, context) # context ì „ë‹¬

        if result.get('fast_fail_triggered', False):
            fast_fail_reason = result.get('fast_fail_reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì´ìœ ')
            context.log.error(f"ğŸ’¥ Fast-Fail ë°œë™: {fast_fail_reason}")
            context.log.error(f"ğŸš¨ ì„±ê³µ: {result['successful_files']}ê°œ, ì‹¤íŒ¨: {result['failed_files']}ê°œ")
            
            for file_result in result['results']:
                if not file_result.success:
                    context.log.error(f"âŒ {Path(file_result.file_path).name}: {file_result.error}")
            
            raise Exception(f"Fast-Fail ë°œë™: {fast_fail_reason}")
        
        if result['failed_files'] > 0:
            context.log.error(f"âŒ {result['failed_files']}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨")
            context.log.error(f"ì„±ê³µ: {result['successful_files']}ê°œ, ì‹¤íŒ¨: {result['failed_files']}ê°œ")
            
            for file_result in result['results']:
                if not file_result.success:
                    context.log.error(f"âŒ {Path(file_result.file_path).name}: {file_result.error}")
            
            raise Exception(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {result['failed_files']}ê°œ íŒŒì¼ì—ì„œ ì˜¤ë¥˜ ë°œìƒ")
        
        context.log.info(f"ğŸ ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {result['successful_files']}ê°œ, ì‹¤íŒ¨ {result['failed_files']}ê°œ")
        context.log.info(f"ğŸ“Š ì´ {result['total_records']:,}ê±´ ì ì¬")
        
        worker_stats = result.get('worker_stats', {})
        if worker_stats:
            context.log.info("ğŸ“Š ì›Œì»¤ë³„ ì„±ëŠ¥ í†µê³„:")
            for worker_id, stats in worker_stats.items():
                context.log.info(f"  ğŸ”¸ {worker_id}: {stats['files_processed']}ê°œ íŒŒì¼, "
                               f"{stats['total_records']:,}ê±´, "
                               f"{stats['total_duration']:.2f}ì´ˆ, "
                               f"{stats['avg_records_per_second']:.0f}ê±´/ì´ˆ")
        
        return dg.MaterializeResult(
            metadata={
                "processed_files": dg.MetadataValue.int(result['successful_files']),
                "failed_files": dg.MetadataValue.int(result['failed_files']),
                "total_records": dg.MetadataValue.int(result['total_records']),
                "max_workers": dg.MetadataValue.int(max_workers),
                "fast_fail_triggered": dg.MetadataValue.bool(result.get('fast_fail_triggered', False)),
                "worker_count": dg.MetadataValue.int(len(worker_stats)),
                "indexes_recreated": dg.MetadataValue.bool(False), # TODO: ë°ì´í„° ì ì¬ í›„ ì¸ë±ìŠ¤ ì¬ìƒì„± ë¡œì§ ì¶”ê°€ í•„ìš”
            }
        )
        
    except Exception as e:
        context.log.error(f"ğŸ’¥ PostgreSQL ì ì¬ ì‹¤íŒ¨: {str(e)}")
        context.log.error(f"ğŸš¨ ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤")
          
        # ëª…ì‹œì ìœ¼ë¡œ ì˜ˆì™¸ë¥¼ ì¬ë°œìƒì‹œì¼œ Dagsterê°€ ì‹¤íŒ¨ë¡œ ì¸ì‹í•˜ë„ë¡ í•¨
        raise

# Asset Check (ê°„ì†Œí™”)
@dg.asset_check(
    asset="nps_to_postgres_simple",
    name="simple_postgres_check", 
    description="PostgreSQL ì—°ê²° ë° ë°ì´í„° í™•ì¸",
    blocking=True
)
def check_postgres_simple(context: dg.AssetCheckExecutionContext, nps_postgres: PostgresResource):
    """ê°„ì†Œí™”ëœ PostgreSQL ì²´í¬ (Fast-Fail ì ìš©)"""
    try:
        context.log.info("ğŸ” PostgreSQL ì—°ê²° ë° í…Œì´ë¸” í™•ì¸ ì¤‘...")

        with nps_postgres.get_connection() as conn:
            with conn.cursor() as cursor:
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                cursor.execute("SELECT 1")
                context.log.info("âœ… PostgreSQL ì—°ê²° ì„±ê³µ")
                
                # í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ë ˆì½”ë“œ ìˆ˜ ì¹´ìš´íŠ¸
                # í…Œì´ë¸” ì´ë¦„ì— ë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì•½ì–´ì™€ ì¶©ëŒ ë°©ì§€ ë° ëŒ€ì†Œë¬¸ì êµ¬ë¶„ (í•„ìš”ì‹œ)
                cursor.execute(f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = '{POSTGRES_NPS_TABLE}'
                    );
                """)
                table_exists = cursor.fetchone()[0]

                if not table_exists:
                    context.log.warning(f"âš ï¸ í…Œì´ë¸” '{POSTGRES_NPS_TABLE}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    return dg.AssetCheckResult(
                        passed=False,
                        description=f"í…Œì´ë¸” '{POSTGRES_NPS_TABLE}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                        metadata={
                            "table_name": dg.MetadataValue.text(POSTGRES_NPS_TABLE),
                            "error_message": dg.MetadataValue.text("Table not found")
                        }
                    )

                cursor.execute(f"SELECT COUNT(*) FROM \"{POSTGRES_NPS_TABLE}\"") # í…Œì´ë¸”ëª…ì— ë”°ì˜´í‘œ ì¶”ê°€
                count = cursor.fetchone()[0]
                context.log.info(f"âœ… í…Œì´ë¸” '{POSTGRES_NPS_TABLE}' í™•ì¸ ì™„ë£Œ: {count:,}ê±´")
                
                return dg.AssetCheckResult(
                    passed=True,
                    metadata={
                        "total_records": dg.MetadataValue.int(count),
                        "table_name": dg.MetadataValue.text(POSTGRES_NPS_TABLE)
                    }
                )
                
    except Exception as e:
        context.log.error(f"ğŸ’¥ PostgreSQL ì²´í¬ ì‹¤íŒ¨: {str(e)}")
        context.log.error(f"ğŸš¨ ì—°ê²° ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”")
        
        return dg.AssetCheckResult(
            passed=False,
            description=f"PostgreSQL ì—°ê²°/í…Œì´ë¸” í™•ì¸ ì‹¤íŒ¨: {str(e)}",
            metadata={
                "error_message": dg.MetadataValue.text(str(e))
            }
        )
