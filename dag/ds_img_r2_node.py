import dagster as dg
import re
import json
import subprocess # Ensured
import os # Ensured
from datetime import datetime # Ensured


def clean_ansi_codes(text: str) -> str:
    """ANSI ì»¬ëŸ¬ ì½”ë“œë¥¼ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë¡œê·¸ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    return ansi_escape.sub('', text)


def extract_crawl_stats(log_text: str) -> dict:
    """í¬ë¡¤ë§ í†µê³„ ì •ë³´ë¥¼ ë¡œê·¸ì—ì„œ ì¶”ì¶œ"""
    stats = {"total_requests": 0, "succeeded": 0, "failed": 0}
    
    # "Total X requests: Y succeeded, Z failed" íŒ¨í„´ ë§¤ì¹­
    pattern = r'Total (\d+) requests: (\d+) succeeded, (\d+) failed'
    match = re.search(pattern, log_text)
    
    if match:
        stats["total_requests"] = int(match.group(1))
        stats["succeeded"] = int(match.group(2))
        stats["failed"] = int(match.group(3))
    
    return stats


@dg.asset(
    group_name="DS", 
    kinds={"source"},
    tags={
        "domain": "image_processing",
        "data_tier": "bronze",
        "source": "node_script"
    },
    description="DS í”„ë¡œì íŠ¸ - Node.js ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ì´ë¯¸ì§€ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë°)" # Updated description
)
def ds_img_r2_node(
    context: dg.AssetExecutionContext
) -> dg.MaterializeResult:
    """DS ì´ë¯¸ì§€ ì²˜ë¦¬ìš© Node.js ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ê³  ë¡œê·¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    
    context.log.info("ğŸš€ DS ì´ë¯¸ì§€ ì²˜ë¦¬ Node.js ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë°)")

    # --- CONFIGURATION: User needs to verify these ---
    command = ["npx", "tsx", "src/main.ts"]
    cwd = "dag/ds_crawlee/ds-naver-crawler" # Assumed path for DS project
    # --- END CONFIGURATION ---

    script_env = os.environ.copy()
    script_env.update({
        "LOG_LEVEL": "DEBUG",
        "DS_PROJECT": "true",
        "IMAGE_PROCESSING": "true",
        "NO_COLOR": "1",
        "FORCE_COLOR": "0",
        "CRAWLEE_MEMORY_MBYTES": "4096",
    })

    log_count_stdout = 0
    log_count_stderr = 0
    processed_images = []
    crawl_stats = {"total_requests": 0, "succeeded": 0, "failed": 0}
    full_stdout_log = []
    full_stderr_log = []
    process = None

    try:
        context.log.info(f"Executing Node.js script: {' '.join(command)} in {cwd}")
        
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            env=script_env,
            cwd=cwd,
            bufsize=1 # Line-buffered
        )

        context.log.info("--- DS Node.js ìŠ¤í¬ë¦½íŠ¸ STDOUT ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ---")
        if process.stdout:
            for line in iter(process.stdout.readline, ''):
                cleaned_line = line.strip()
                context.log.info(cleaned_line)
                log_count_stdout += 1
                full_stdout_log.append(cleaned_line)
                
                if "Company logo saved:" in cleaned_line:
                    filename_match = re.search(r'clogo-[a-f0-9-]+\.png', cleaned_line)
                    if filename_match:
                        filename = filename_match.group()
                        processed_images.append(filename)
                        context.log.info(f"âœ… DS ë¡œê³  ì €ì¥ ì™„ë£Œ (from stream): {filename}")
            process.stdout.close()
        context.log.info("--- DS Node.js ìŠ¤í¬ë¦½íŠ¸ STDOUT ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ---")

        context.log.info("--- DS Node.js ìŠ¤í¬ë¦½íŠ¸ STDERR ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ---")
        if process.stderr:
            for line in iter(process.stderr.readline, ''):
                cleaned_line = line.strip()
                context.log.error(cleaned_line)
                log_count_stderr += 1
                full_stderr_log.append(cleaned_line)
            process.stderr.close()
        context.log.info("--- DS Node.js ìŠ¤í¬ë¦½íŠ¸ STDERR ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ---")

        return_code = process.wait()

        stdout_full_text = "\n".join(full_stdout_log)
        stats_from_stdout = extract_crawl_stats(clean_ansi_codes(stdout_full_text))
        if stats_from_stdout["total_requests"] > 0:
            crawl_stats = stats_from_stdout
            context.log.info(f"ğŸ“ˆ DS í¬ë¡¤ë§ í†µê³„ (STDOUT ì „ì²´ ë¶„ì„): ì´ {crawl_stats['total_requests']}ê°œ, ì„±ê³µ {crawl_stats['succeeded']}ê°œ, ì‹¤íŒ¨ {crawl_stats['failed']}ê°œ")
        
        success = return_code == 0
        context.log.info(f"=== âœ… DS ì´ë¯¸ì§€ ì²˜ë¦¬ Node.js ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ (exit_code: {return_code}) ===")

        if not success:
            error_message = f"DS Node.js script '{' '.join(command)}' failed with exit code {return_code} in {cwd}."
            context.log.error(error_message)
            if full_stderr_log:
                context.log.error("--- DS ì „ì²´ STDERR ë‚´ìš© ---")
                for err_line in full_stderr_log:
                    context.log.error(err_line)
            raise Exception(error_message)

        if processed_images:
            context.log.info(f"ğŸ“Š ì´ {len(processed_images)}ê°œ DS íšŒì‚¬ ë¡œê³  ì²˜ë¦¬ ì™„ë£Œ:")
            for img in processed_images:
                context.log.info(f"  ğŸ“¸ {img}")
        
        count_value = max(crawl_stats.get("succeeded", 0), len(processed_images))
        
        return dg.MaterializeResult(
            metadata={
                "dagster/row_count": dg.MetadataValue.int(count_value),
                "script_command": dg.MetadataValue.text(" ".join(command)),
                "script_cwd": dg.MetadataValue.text(cwd),
                "exit_code": dg.MetadataValue.int(return_code),
                "log_lines_stdout": dg.MetadataValue.int(log_count_stdout),
                "log_lines_stderr": dg.MetadataValue.int(log_count_stderr),
                "processed_images_count": dg.MetadataValue.int(len(processed_images)),
                "processed_images": dg.MetadataValue.json(processed_images),
                "crawl_total_requests": dg.MetadataValue.int(crawl_stats["total_requests"]),
                "crawl_succeeded": dg.MetadataValue.int(crawl_stats["succeeded"]),
                "crawl_failed": dg.MetadataValue.int(crawl_stats["failed"]),
                "success": dg.MetadataValue.bool(success),
                "execution_method": dg.MetadataValue.text("subprocess.Popen_node.js_streaming"),
                "processing_timestamp": dg.MetadataValue.text(datetime.now().isoformat()),
                "project": dg.MetadataValue.text("DS")
            }
        )
        
    except subprocess.TimeoutExpired as te: # Popen.wait()ì— timeout ì„¤ì • ì‹œ í•„ìš”
        context.log.error(f"âŒ DS ì´ë¯¸ì§€ ì²˜ë¦¬ Node.js ìŠ¤í¬ë¦½íŠ¸ ì‹œê°„ ì´ˆê³¼: {te}")
        if process: process.kill()
        if hasattr(te, 'stdout') and te.stdout:
            context.log.error(f"STDOUT on timeout (DS): {te.stdout.decode(errors='ignore')}")
        if hasattr(te, 'stderr') and te.stderr:
            context.log.error(f"STDERR on timeout (DS): {te.stderr.decode(errors='ignore')}")
        raise
    except FileNotFoundError:
        context.log.error(f"âŒ DS Node.js ìŠ¤í¬ë¦½íŠ¸ ë˜ëŠ” 'npx'/'tsx' ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Command: {' '.join(command)}, CWD: {cwd}. PATHì™€ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        raise
    except Exception as e:
        context.log.error(f"âŒ DS ì´ë¯¸ì§€ ì²˜ë¦¬ Node.js ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        if full_stdout_log:
            stdout_excerpt = "\n".join(full_stdout_log)[-1000:]
            context.log.error(f"Captured STDOUT so far (DS): {stdout_excerpt}")
        if full_stderr_log:
            stderr_excerpt = "\n".join(full_stderr_log)[-1000:]
            context.log.error(f"Captured STDERR so far (DS): {stderr_excerpt}")
        raise
    finally:
        if process:
            if process.stdout and not process.stdout.closed:
                process.stdout.close()
            if process.stderr and not process.stderr.closed:
                process.stderr.close()
            if process.poll() is None:
                try:
                    process.terminate()
                    process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    process.kill()
                    process.wait()
                except Exception as e_finally:
                    context.log.error(f"Error during DS process cleanup: {e_finally}")

# ... (rest of the file, if any)
