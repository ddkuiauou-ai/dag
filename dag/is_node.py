"""
IS (Isshoo) í”„ë¡œì íŠ¸ - ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ë§ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ í•œêµ­ì˜ ì£¼ìš” ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ë“¤ì„ í¬ë¡¤ë§í•˜ì—¬ ê²Œì‹œë¬¼ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
Node.js ê¸°ë°˜ì˜ Crawlee í”„ë ˆì„ì›Œí¬ì™€ Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ì„±ëŠ¥ í¬ë¡¤ë§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
- ğŸš€ ë¹Œë“œëœ JavaScript íŒŒì¼ ì‚¬ìš©ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥
- ğŸ“Š ë””ë ‰í† ë¦¬ ìŠ¤ìº” ê¸°ë°˜ í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰
- ğŸ”„ ì¦ë¶„ í¬ë¡¤ë§ ì§€ì› (ì¤‘ë³µ ë°ì´í„° ë°©ì§€)
- ğŸ›¡ï¸ ê²¬ê³ í•œ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
- ğŸ“ˆ ìƒì„¸í•œ ì‹¤í–‰ í†µê³„ ë° ë©”íƒ€ë°ì´í„°

ì§€ì› ì‚¬ì´íŠ¸:
- ë‹¤ëª¨ì•™ (Damoang): ì •ì¹˜/ì‚¬íšŒ ì»¤ë®¤ë‹ˆí‹°
- í´ë¦¬ì•™ (Clien): IT/ê¸°ìˆ  ì»¤ë®¤ë‹ˆí‹° (í–¥í›„ ì§€ì› ì˜ˆì •)
- ê¸°íƒ€ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ëŠ” ë””ë ‰í† ë¦¬ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ íƒìƒ‰ë©ë‹ˆë‹¤.

ì•„í‚¤í…ì²˜:
1. is_crawler_build: TypeScript â†’ JavaScript ë¹Œë“œ ê´€ë¦¬
2. is_crawler_executor: ë””ë ‰í† ë¦¬ ë‚´ Crawlee JS ìˆœì°¨ ì‹¤í–‰
3. ì¶œë ¥: JSON íŒŒì¼ë¡œ êµ¬ì¡°í™”ëœ ê²Œì‹œë¬¼ ë°ì´í„° ì €ì¥

ì‘ì„±ì: DAG í”„ë¡œì íŠ¸ íŒ€
ìµœì¢… ìˆ˜ì •: 2025-06-19
ë³‘ë ¬ ì‹¤í–‰: Dynamic Mapping ê¸°ë°˜ is_crawler_parallel_job ì¶”ê°€ (2025-08-13)
"""

import json
import os
import re
import hashlib
import subprocess
from datetime import datetime
from pathlib import Path

import dagster as dg

from dagster import schedule, RunRequest, op, job, DynamicOut, DynamicOutput

# ===== Performance: shared constants, precompiled regex, and helpers =====
from typing import Tuple

FALLBACK_CRAWLER_PATH = Path("./dag/is_crawlee")

# Centralized timeouts (seconds)
BUILD_TIMEOUT_SEC = 300
SEQUENTIAL_TIMEOUT_SEC = 1800

# Common env used for all subprocess runs
COMMON_ENV = {**os.environ, "NODE_ENV": "production", "NO_COLOR": "1"}

# Precompiled regex to avoid recompilation overhead per call
ANSI_ESCAPE_RE = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
FINISHED_RE = re.compile(r'Finished!\s+Total\s+(\d+)\s+requests:\s+(\d+)\s+succeeded,\s+(\d+)\s+failed\.\s*(\{.*?\})?')
FINAL_STATS_RE = re.compile(r'Final request statistics: (\{.*?\})')
DETAIL_RE = re.compile(r'ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬')
MIME_RE = re.compile(r"(?:\bmimeType\b|\bmime_type\b)\s*[:=]\s*['\"]?([\w.-]+\/[\w.+-]+)", re.IGNORECASE)


def _resolve_base_path() -> Path:
    """Return the actual base crawlee path with a robust fallback."""
    return BASE_CRAWLER_PATH if BASE_CRAWLER_PATH.exists() else FALLBACK_CRAWLER_PATH


def _iter_crawler_dirs_sorted() -> list[Path]:
    """Single-level scan of crawler directories, sorted by name."""
    base = _resolve_base_path()
    return sorted([d for d in base.iterdir() if d.is_dir()], key=lambda d: d.name)


def _summarize_logs(name: str, stdout: str | bytes, stderr: str | bytes, duration: float, timeout_seconds: int | None = None) -> Tuple[dict, bool]:
    """Common log cleaner + stat extractor used by all runners.
    Returns (out_dict, finished_ok).
    """
    clean_stdout = clean_ansi_codes(stdout)
    clean_stderr = clean_ansi_codes(stderr)
    stats = extract_crawl_stats(clean_stdout)
    finished_ok = (
        stats.get("failed") == 0
        and (stats.get("total_requests") or 0) > 0
        and (stats.get("succeeded") or 0) >= (stats.get("total_requests") or 0)
    )

    out = {
        "name": name,
        "status": "success",  # caller may override
        "terminal": bool(stats.get("terminal")) if "terminal" in stats else None,
        "requests": stats.get("total_requests"),
        "succeeded": stats.get("succeeded"),
        "failed": stats.get("failed"),
        "posts_collected": stats.get("posts_collected"),
        "embedded_total": stats.get("embedded_total"),
        "embedded_by_mime": stats.get("embedded_by_mime"),
        "duration_seconds": duration,
        "timeout_seconds": timeout_seconds,
        "log_stdout_snippet": (clean_stdout[-500:] if clean_stdout else ""),
        "log_stderr_snippet": (clean_stderr[-500:] if clean_stderr else ""),
    }
    return out, finished_ok


# í¬ë¡¤ëŸ¬ ë¹Œë“œ ë˜ëŠ” ì‹¤í–‰ ì¤‘ì— ë°œìƒí•˜ëŠ” ì˜ˆì™¸ì…ë‹ˆë‹¤.
class CrawlerExecutionError(Exception):
    """í¬ë¡¤ëŸ¬ ë¹Œë“œ ë˜ëŠ” ì‹¤í–‰ ì¤‘ì— ë°œìƒí•˜ëŠ” ì˜ˆì™¸ì…ë‹ˆë‹¤."""
    pass



# (Duplicate regex definitions removed here)


def clean_ansi_codes(text: str | bytes) -> str:
    """
    ANSI ì»¬ëŸ¬ ì½”ë“œë¥¼ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë¡œê·¸ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Crawleeì™€ Playwrightì—ì„œ ì¶œë ¥ë˜ëŠ” ì»¬ëŸ¬ ì½”ë“œë¥¼ ì œê±°í•˜ì—¬
    Dagster ë¡œê·¸ì—ì„œ ì½ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.

    Args:
        text: ANSI ì½”ë“œê°€ í¬í•¨ëœ ì›ë³¸ í…ìŠ¤íŠ¸ (str ë˜ëŠ” bytes)

    Returns:
        ANSI ì½”ë“œê°€ ì œê±°ëœ ê¹”ë”í•œ í…ìŠ¤íŠ¸
    """
    if isinstance(text, (bytes, bytearray)):
        try:
            text = text.decode("utf-8", errors="ignore")
        except Exception:
            # í˜¹ì‹œ ëª¨ë¥¼ ë””ì½”ë”© ì´ìŠˆì— ëŒ€ë¹„
            text = str(text)
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    return ansi_escape.sub('', text)


def extract_crawl_stats(log_text: str) -> dict[str, int]:
    """
    í¬ë¡¤ë§ í†µê³„ ì •ë³´ë¥¼ ë¡œê·¸ì—ì„œ ì¶”ì¶œ
    Crawlee ë¡œê·¸ì—ì„œ ìš”ì²­ í†µê³„ì™€ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ íšŸìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì§‘ ê±´ìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë˜í•œ ë¡œê·¸ ë‚´ ì„ë² ë””ë“œ(embedded) í•­ëª©ì˜ mimeType ë¹ˆë„ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤.
    """
    stats = {
        "total_requests": 0,
        "succeeded": 0,
        "failed": 0,
        "posts_collected": 0,
        "embedded_total": 0,
        "embedded_by_mime": {},
        "terminal": None,
    }
    # Also parse the finishing summary line to reduce false negatives
    finished_match = re.search(r'Finished!\s+Total\s+(\d+)\s+requests:\s+(\d+)\s+succeeded,\s+(\d+)\s+failed\.\s*(\{.*?\})?', log_text)
    if finished_match:
        try:
            total = int(finished_match.group(1))
            succ = int(finished_match.group(2))
            fail = int(finished_match.group(3))
            # Prefer explicit finished numbers when present
            stats["total_requests"] = max(stats.get("total_requests", 0), total)
            stats["succeeded"] = succ
            stats["failed"] = fail
            # Parse optional trailing JSON for terminal flag
            tail = finished_match.group(4)
            if tail:
                try:
                    tail_json = json.loads(tail)
                    stats["terminal"] = bool(tail_json.get("terminal"))
                except Exception:
                    stats["terminal"] = None
            else:
                stats["terminal"] = None
        except Exception:
            pass

    # Final statistics JSON íŒŒì‹±
    final_pattern = r'Final request statistics: (\{.*?\})'
    final_match = re.search(final_pattern, log_text)
    if final_match:
        try:
            stats_json = json.loads(final_match.group(1))
            stats["total_requests"] = stats_json.get("requestsFinished", 0)
            stats["succeeded"] = stats_json.get("requestsFinished", 0) - stats_json.get("requestsFailed", 0)
            stats["failed"] = stats_json.get("requestsFailed", 0)
        except Exception:
            # í†µê³„ JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ì§„í–‰
            pass

    # ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ íšŸìˆ˜ ì¹´ìš´íŠ¸
    detail_pattern = r'ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬'
    stats["posts_collected"] = len(re.findall(detail_pattern, log_text))

    # embedded mimeType ì¹´ìš´íŠ¸: JSON/ë¡œê·¸ í˜•íƒœ ëª¨ë‘ í—ˆìš© (í‚¤/ê°’ ìˆœì„œ ë¶ˆë¬¸, ì‘ì€ë”°ì˜´í‘œ/í°ë”°ì˜´í‘œ í˜¼ìš© í—ˆìš©)
    # ì˜ˆì‹œ ë§¤ì¹˜ ëŒ€ìƒ:
    #  - { "embedded": [{ "mimeType": "image/png", ... }] }
    #  - mimeType: 'application/pdf'
    #  - mime_type = "text/html"
    mime_regex = re.compile(r"(?:\bmimeType\b|\bmime_type\b)\s*[:=]\s*['\"]?([\w.-]+\/[\w.+-]+)", re.IGNORECASE)
    mime_counts: dict[str, int] = {}
    for match in mime_regex.finditer(log_text):
        mime = match.group(1).lower()
        mime_counts[mime] = mime_counts.get(mime, 0) + 1

    stats["embedded_by_mime"] = mime_counts
    stats["embedded_total"] = sum(mime_counts.values())

    return stats


# =========================
# Dagster-native parallelization (Dynamic Mapping + Collect)
# =========================

BASE_CRAWLER_PATH = Path("/Users/craigchoi/silla/dag/dag/is_crawlee")

@op
def build_crawlers(context: dg.OpExecutionContext) -> int:
    """Build all Crawlee projects (op version of is_crawler_build).
    Returns the number of crawler directories built. This op exists so that
    the parallel run job can depend on a build step within the same run.
    """
    context.log.info("ğŸ”¨ [build_crawlers] ì‹œì‘")
    base_path = BASE_CRAWLER_PATH if BASE_CRAWLER_PATH.exists() else Path("./dag/is_crawlee")
    crawler_dirs = [d for d in base_path.iterdir() if d.is_dir()]
    if not crawler_dirs:
        context.log.warning("âš ï¸ ë¹Œë“œ ëŒ€ìƒ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: %s", base_path)
        return 0

    build_command = ["npm", "run", "build"]
    timeout_seconds = 300

    for crawler_dir in crawler_dirs:
        context.log.info(f"ğŸ”¨ ë¹Œë“œ: {crawler_dir.name}")
        result = subprocess.run(
            build_command,
            cwd=str(crawler_dir),
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            env={**os.environ, "NODE_ENV": "production"}
        )
        if result.returncode != 0:
            context.log.error(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {crawler_dir.name} (ì½”ë“œ: {result.returncode})")
            context.log.error(result.stdout)
            context.log.error(result.stderr)
            raise CrawlerExecutionError(f"{crawler_dir.name} ë¹Œë“œ ì‹¤íŒ¨: {result.stderr or result.stdout}")
    context.log.info(f"âœ… ë¹Œë“œ ì™„ë£Œ: {len(crawler_dirs)}ê°œ")
    return len(crawler_dirs)



# Helper: produce valid Dagster mapping keys from arbitrary directory names
def _to_valid_mapping_key(name: str) -> str:
    """Convert arbitrary directory names to a Dagster-valid mapping_key.
    Dagster requires names to match ^[A-Za-z0-9_]+$ for mapping keys.
    We replace invalid characters with `_`, collapse repeats, and trim.
    """
    # Replace any non [A-Za-z0-9_] with '_'
    key = re.sub(r"[^A-Za-z0-9_]", "_", name)
    # Collapse multiple underscores and strip edges
    key = re.sub(r"_+", "_", key).strip("_")
    # Ensure non-empty
    if not key:
        key = "item"
    # Optional: cap length to avoid unwieldy op labels in UI
    return key[:128]


@op(out=DynamicOut(str))
def list_crawler_dirs(context: dg.OpExecutionContext, _built_count: int) -> DynamicOutput[str]:
    """ìƒìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” â†’ ê° í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ DynamicOutputìœ¼ë¡œ ë°©ì¶œ."""
    base_path = BASE_CRAWLER_PATH if BASE_CRAWLER_PATH.exists() else Path("./dag/is_crawlee")
    crawler_dirs = sorted([d for d in base_path.iterdir() if d.is_dir()], key=lambda d: d.name)
    if not crawler_dirs:
        context.log.warning("âš ï¸ ì‹¤í–‰ ëŒ€ìƒ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: %s", base_path)
    # Ensure mapping_key uniqueness after sanitization
    seen: set[str] = set()
    for d in crawler_dirs:
        raw = d.name
        key = _to_valid_mapping_key(raw)
        if key in seen:
            # Append short hash of original name to disambiguate collisions
            suffix = hashlib.sha1(raw.encode("utf-8")).hexdigest()[:6]
            key = f"{key}_{suffix}"
        seen.add(key)
        context.log.info(f"ğŸ“„ ëŒ€ìƒ: {raw} (mapping_key={key})")
        yield DynamicOutput(str(d), mapping_key=key)


@op(config_schema={"args": [str]}, pool="node_crawlers")
def run_crawler(context: dg.OpExecutionContext, crawler_dir: str) -> dict:
    """í•œ ê°œì˜ Crawlee ë¹Œë“œ ì‚°ì¶œë¬¼(dist/main.js)ì„ ì‹¤í–‰.
    - pool="node_crawlers": ë™ì‹œ ì‹¤í–‰ ê°œìˆ˜ë¥¼ ë°°í¬ ì„¤ì •ì—ì„œ ì œì–´í•©ë‹ˆë‹¤.
    - config_schema.args: ìŠ¤ì¼€ì¤„ì—ì„œ ì „ë‹¬ë˜ëŠ” ê°„ê²© íŒŒë¼ë¯¸í„° (ì˜ˆ: ["--hours", "3"]).
    ë°˜í™˜ê°’ì€ ì§‘ê³„ë¥¼ ìœ„í•´ í•„ìš”í•œ ìµœì†Œ í†µê³„ë§Œ í¬í•¨í•©ë‹ˆë‹¤.
    """
    # ğŸ”§ Parallel timeout: single source of truth
    PARALLEL_TIMEOUT_SEC = 3000  # 50 minutes for parallel run_crawler
    TIMEOUT_ERROR_MSG = f"{PARALLEL_TIMEOUT_SEC // 60}ë¶„ íƒ€ì„ì•„ì›ƒ ë°œìƒ"

    args = context.op_config.get("args") or []
    cmd = ["node", "dist/main.js", "--mode", "full"] + [str(a) for a in args]
    env = {**os.environ, "NODE_ENV": "production", "NO_COLOR": "1"}

    context.log.info(f"â–¶ ì‹¤í–‰({Path(crawler_dir).name}): {cmd}")
    start_time = datetime.now()

    try:
        result = subprocess.run(
            cmd,
            cwd=str(crawler_dir),
            capture_output=True,
            text=True,
            timeout=PARALLEL_TIMEOUT_SEC,
            env=env,
        )
        duration = (datetime.now() - start_time).total_seconds()

        clean_stdout = clean_ansi_codes(result.stdout)
        clean_stderr = clean_ansi_codes(result.stderr)
        stats = extract_crawl_stats(clean_stdout)
        finished_ok = (
            stats.get("failed") == 0
            and (stats.get("total_requests") or 0) > 0
            and (stats.get("succeeded") or 0) >= (stats.get("total_requests") or 0)
        )
        terminal = bool(stats.get("terminal")) if "terminal" in stats else None

        out = {
            "name": Path(crawler_dir).name,
            "status": "success" if result.returncode == 0 or finished_ok else "failure",
            "terminal": terminal,
            "requests": stats.get("total_requests"),
            "succeeded": stats.get("succeeded"),
            "failed": stats.get("failed"),
            "posts_collected": stats.get("posts_collected"),
            "embedded_total": stats.get("embedded_total"),
            "embedded_by_mime": stats.get("embedded_by_mime"),
            "duration_seconds": duration,
            "timeout_seconds": PARALLEL_TIMEOUT_SEC,
            "log_stdout_snippet": (clean_stdout[-500:] if clean_stdout else ""),
            "log_stderr_snippet": (clean_stderr[-500:] if clean_stderr else ""),
        }

        # ì‹¤íŒ¨ ì¡°ê±´: í”„ë¡œì„¸ìŠ¤ ë¹„ì •ìƒ ì¢…ë£Œì´ë©´ì„œ ëª…í™•í•œ ì™„ë£Œ ì‹ í˜¸ê°€ ì—†ëŠ” ê²½ìš° â†’ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ì—¬ Run ì‹¤íŒ¨ ì²˜ë¦¬
        if result.returncode != 0 and not finished_ok:
            context.log.error(
                f"ğŸ’¥ ì‹¤íŒ¨: {out['name']} â€” {out['log_stderr_snippet'] or out['log_stdout_snippet']}"
            )
            # ìš”ì•½ ë¡œê·¸ ë‚¨ê¸°ê³  Dagster Failure ë°œìƒì‹œì¼œ step ì‹¤íŒ¨ë¡œ ë§ˆí‚¹
            context.log.info(
                "ğŸ“Š [%s] status=%s posts=%s req=%s ok=%s fail=%s embed=%s dur=%.1fs",
                out.get("name"),
                out.get("status"),
                out.get("posts_collected") or 0,
                out.get("requests") or 0,
                out.get("succeeded") or 0,
                out.get("failed") or 0,
                out.get("embedded_total") or 0,
                (out.get("duration_seconds") or 0.0),
            )
            raise dg.Failure(
                description=f"í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {out['name']}",
                metadata={
                    **out,
                    "error_type": "process_exit",
                    "error_message": out.get("log_stderr_snippet") or out.get("log_stdout_snippet") or "non-zero exit",
                },
            )

        # Per-crawler one-line summary for easy scanning in logs
        context.log.info(
            "ğŸ“Š [%s] status=%s posts=%s req=%s ok=%s fail=%s embed=%s dur=%.1fs",
            out.get("name"),
            out.get("status"),
            out.get("posts_collected") or 0,
            out.get("requests") or 0,
            out.get("succeeded") or 0,
            out.get("failed") or 0,
            out.get("embedded_total") or 0,
            (out.get("duration_seconds") or 0.0),
        )
        return out

    except subprocess.TimeoutExpired as e:
        # Ensure timeouts don't crash the whole parallel map; record and continue aggregation
        duration = (datetime.now() - start_time).total_seconds()
        raw_out = getattr(e, "output", None) or getattr(e, "stdout", None) or b""
        raw_err = getattr(e, "stderr", None) or b""
        clean_stdout = clean_ansi_codes(raw_out)
        clean_stderr = clean_ansi_codes(raw_err)
        stats = extract_crawl_stats(clean_stdout)
        timeout_msg = TIMEOUT_ERROR_MSG

        out = {
            "name": Path(crawler_dir).name,
            "status": "timeout",
            "terminal": None,
            "requests": stats.get("total_requests"),
            "succeeded": stats.get("succeeded"),
            "failed": stats.get("failed"),
            "posts_collected": stats.get("posts_collected"),
            "embedded_total": stats.get("embedded_total"),
            "embedded_by_mime": stats.get("embedded_by_mime"),
            "duration_seconds": duration,
            "timeout_seconds": PARALLEL_TIMEOUT_SEC,
            "error_message": timeout_msg,
            "log_stdout_snippet": (clean_stdout[-500:] if clean_stdout else ""),
            "log_stderr_snippet": (clean_stderr[-500:] if clean_stderr else ""),
        }

        context.log.error(f"â° íƒ€ì„ì•„ì›ƒ: {out['name']} â€” {timeout_msg}")
        # ìš”ì•½ ë¡œê·¸ëŠ” ë‚¨ê¸°ë˜, Dagster Failureë¥¼ ë°œìƒì‹œì¼œ stepì„ ì‹¤íŒ¨ë¡œ í‘œì‹œ
        context.log.info(
            "ğŸ“Š [%s] status=%s posts=%s req=%s ok=%s fail=%s embed=%s dur=%.1fs",
            out.get("name"),
            out.get("status"),
            out.get("posts_collected") or 0,
            out.get("requests") or 0,
            out.get("succeeded") or 0,
            out.get("failed") or 0,
            out.get("embedded_total") or 0,
            (out.get("duration_seconds") or 0.0),
        )
        raise dg.Failure(
            description=f"íƒ€ì„ì•„ì›ƒ: {out['name']} â€” {timeout_msg}",
            metadata={
                **out,
                "error_type": "timeout",
                "error_message": timeout_msg,
            },
        )


@op
def aggregate_crawler_results(context: dg.OpExecutionContext, results: list[dict]) -> dict:
    """Dynamic map ê²°ê³¼ ì§‘ê³„ (ê¸°ì¡´ is_crawler_executor ì§‘ê³„ ë¡œì§ê³¼ ë™ì¼í•œ í•„ë“œ ìœ ì§€)."""
    total_requests = sum((r.get("requests") or 0) for r in results)
    total_succeeded = sum((r.get("succeeded") or 0) for r in results)
    total_failed = sum((r.get("failed") or 0) for r in results)
    total_posts = sum((r.get("posts_collected") or 0) for r in results)

    from collections import Counter
    embedded_counter = Counter()
    for r in results:
        for k, v in (r.get("embedded_by_mime") or {}).items():
            embedded_counter[k] += v
    total_embedded = sum(embedded_counter.values())

    total_duration = sum((r.get("duration_seconds") or 0) for r in results)
    avg_requests_per_second = total_requests / total_duration if total_duration > 0 else 0
    avg_posts_per_second = total_posts / total_duration if total_duration > 0 else 0

    summary = {
        "total_crawlers": len(results),
        "total_requests": total_requests,
        "total_succeeded": total_succeeded,
        "total_failed": total_failed,
        "posts_collected": total_posts,
        "average_requests_per_second": round(avg_requests_per_second, 2),
        "average_posts_per_second": round(avg_posts_per_second, 2),
        "embedded_total": total_embedded,
        "embedded_by_mime": dict(embedded_counter),
        "per_crawler_stats": results,
    }
    # Per-crawler summaries
    context.log.info("â”€â”€â”€â”€â”€â”€â”€â”€ per-crawler summary â”€â”€â”€â”€â”€â”€â”€â”€")
    for r in results:
        try:
            context.log.info(
                "â€¢ %s â†’ status=%s posts=%s req=%s ok=%s fail=%s embed=%s dur=%.1fs",
                r.get("name"),
                r.get("status"),
                r.get("posts_collected") or 0,
                r.get("requests") or 0,
                r.get("succeeded") or 0,
                r.get("failed") or 0,
                r.get("embedded_total") or 0,
                (r.get("duration_seconds") or 0.0),
            )
        except Exception:
            # ë°©ì–´ì  ë¡œê¹…: í•œ í•­ëª© ë¬¸ì œë¡œ ì „ì²´ ë¡œê·¸ê°€ ê¹¨ì§€ì§€ ì•Šë„ë¡
            context.log.debug(f"per-crawler log skipped for: {r}")

    context.log.info(f"âœ… ë³‘ë ¬ ì‹¤í–‰ ì™„ë£Œ: {len(results)}ê°œ, ì´ ê²Œì‹œë¬¼ {total_posts}ê°œ")
    return summary


@job
def is_crawler_parallel_job():
    """Dagster-native ë³‘ë ¬ ì‹¤í–‰ ì¡.
    1) build_crawlers â†’ 2) list_crawler_dirs (DynamicOut) â†’ 3) run_crawler(map) â†’ 4) aggregate
    """
    built = build_crawlers()
    dirs = list_crawler_dirs(built)
    results = dirs.map(run_crawler)
    aggregate_crawler_results(results.collect())




@dg.asset(
    group_name="IS",
    kinds={"build"},
    tags={
        "domain": "community_content",
        "process": "build",
        "technology": "typescript",
        "framework": "crawlee",
        "tier": "infrastructure"
    },
    description="IS í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  Crawlee TypeScript ì½”ë“œë¥¼ JavaScriptë¡œ ë¹Œë“œí•©ë‹ˆë‹¤."
)
def is_crawler_build(
    context: dg.AssetExecutionContext
) -> dg.MaterializeResult:
    """
    IS í¬ë¡¤ëŸ¬ ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  Crawlee í”„ë¡œì íŠ¸ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤.

    ì´ Assetì€ ì§€ì •ëœ ìƒìœ„ í´ë”ë¥¼ ë‹¨ì¼ ë ˆë²¨ë¡œ ìŠ¤ìº”í•˜ì—¬, ê° í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ì—ì„œ
    `npm run build` ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ JavaScript íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ë¹Œë“œ ê³¼ì •:
    1. ìµœìƒìœ„ í•œ ë ˆë²¨ì˜ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ëª©ë¡ ìˆ˜ì§‘
    2. ê° ë””ë ‰í† ë¦¬ì—ì„œ `npm run build` ì‹¤í–‰ (5ë¶„ íƒ€ì„ì•„ì›ƒ)
    3. ëª¨ë“  í¬ë¡¤ëŸ¬ ë¹Œë“œ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
    4. ë¹Œë“œëœ í¬ë¡¤ëŸ¬ ìˆ˜ë¥¼ `dagster/row_count` ë©”íƒ€ë°ì´í„°ë¡œ ë°˜í™˜

    Returns:
        MaterializeResult: ë¹Œë“œ ë©”íƒ€ë°ì´í„°
            - build_success: ì „ì²´ ë¹Œë“œ ì„±ê³µ ì—¬ë¶€ (bool)
            - dagster/row_count: ë¹Œë“œëœ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìˆ˜ (int)

    Raises:
        CrawlerExecutionError: ë¹Œë“œ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ
    """

    context.log.info("ğŸ”¨ IS í¬ë¡¤ëŸ¬ ë¹Œë“œ ì‹œì‘ - ìƒìœ„ ë‹¨ì¼ ë ˆë²¨ ìŠ¤ìº”")
    build_start_time = datetime.now()

    # ë¹Œë“œí•  í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìƒìœ„ ê²½ë¡œ (ë‹¨ì¼ ë ˆë²¨ ìŠ¤ìº”)
    base_path = Path("/Users/craigchoi/silla/dag/dag/is_crawlee")
    if not base_path.exists():
        base_path = Path("./dag/is_crawlee")
    context.log.info(f"ğŸ” í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ íƒìƒ‰ ê²½ë¡œ: {base_path}")

    crawler_dirs = [d for d in base_path.iterdir() if d.is_dir()]
    context.log.info(f"ğŸ” ë°œê²¬ëœ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬: {[d.name for d in crawler_dirs]}")

    build_command = ["npm", "run", "build"]
    timeout_seconds = 300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ

    for crawler_dir in crawler_dirs:
        context.log.info(f"ğŸ”¨ ë¹Œë“œ ì‹œì‘: {crawler_dir.name}")
        result = subprocess.run(
            build_command,
            cwd=str(crawler_dir),
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            env={**os.environ, "NODE_ENV": "production"}
        )
        if result.returncode != 0:
            context.log.error(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {crawler_dir.name} (ì½”ë“œ: {result.returncode})")
            context.log.error(result.stdout)
            context.log.error(result.stderr)
            raise CrawlerExecutionError(f"{crawler_dir.name} ë¹Œë“œ ì‹¤íŒ¨: {result.stderr or result.stdout}")

    build_duration = (datetime.now() - build_start_time).total_seconds()
    built_count = len(crawler_dirs)
    context.log.info(f"âœ… ëª¨ë“  í¬ë¡¤ëŸ¬ ë¹Œë“œ ì„±ê³µ! ë¹Œë“œ ê°œìˆ˜: {built_count}, ì†Œìš” ì‹œê°„: {build_duration:.2f}ì´ˆ")

    return dg.MaterializeResult(
        metadata={
            "build_success": True,
            "dagster/row_count": built_count
        }
    )


@dg.asset(
    deps=[is_crawler_build],  # ë¹Œë“œ Assetì— ì˜ì¡´
    group_name="IS",
    kinds={"source"},
    tags={
        "domain": "community_content",
        "data_tier": "bronze",
        "source": "node_script",
        "technology": "crawlee",
        "framework": "playwright",
        "extraction_type": "web_scraping"
    },
    description="IS í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  Crawlee ë¹Œë“œëœ JSë¥¼ ìˆœì°¨ ì‹¤í–‰í•˜ê³ , ë¡œê·¸ ê¸°ë°˜ í†µê³„ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
)
def is_crawler_executor(
    context: dg.AssetExecutionContext
) -> dg.MaterializeResult:
    """
    ë””ë ‰í† ë¦¬ ê¸°ë°˜ Crawlee JS ì‹¤í–‰ ë° ë¡œê·¸ ê¸°ë°˜ í†µê³„ ìˆ˜ì§‘ Asset.

    ì´ Assetì€ ë¹Œë“œëœ Crawlee JavaScript íŒŒì¼ì´ ìˆëŠ” ê° í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ë¥¼ ë‹¨ì¼ ë ˆë²¨ë¡œ ìŠ¤ìº”í•˜ì—¬,
    ìˆœì°¨ì ìœ¼ë¡œ `node dist/main.js`ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ì‹¤í–‰ í›„ ë¡œê·¸ì˜ ìµœì¢… ìš”ì²­ í†µê³„ì™€
    ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ íšŸìˆ˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì‹¤í–‰ ê³¼ì •:
    1. ìƒìœ„ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ë‹¨ì¼ ë ˆë²¨ ìŠ¤ìº”
    2. ê° ë””ë ‰í† ë¦¬ë³„ `node dist/main.js` ì‹¤í–‰ (30ë¶„ íƒ€ì„ì•„ì›ƒ)
    3. Crawlee ë¡œê·¸ì˜ "Final request statistics" JSON íŒŒì‹±:
       - requestsFinished â†’ total_requests
       - requestsFailed â†’ failed, succeeded = total_requests - failed
    4. ë¡œê·¸ì—ì„œ "ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬" ë°œìƒ íšŸìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•˜ì—¬ posts_collectedë¡œ ì„¤ì •
    5. í¬ë¡¤ëŸ¬ë³„ ë° ì „ì²´ í†µê³„ ì§‘ê³„ í›„ ë°˜í™˜

    Returns:
        MaterializeResult: ì‹¤í–‰ í†µê³„ ë° í¬ë¡¤ëŸ¬ë³„ ìƒì„¸ ì •ë³´ í¬í•¨ ë©”íƒ€ë°ì´í„°
        - total_crawlers: ì‹¤í–‰ëœ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìˆ˜ (int)
        - total_requests: ì´ ìš”ì²­ ìˆ˜ (int)
        - succeeded: ì„±ê³µ ìš”ì²­ ìˆ˜ (int)
        - failed: ì‹¤íŒ¨ ìš”ì²­ ìˆ˜ (int)
        - posts_collected: ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ê±´ìˆ˜ í•©ê³„ (int)
        - per_crawler_stats: ê°œë³„ í¬ë¡¤ëŸ¬ë³„ í†µê³„ ë¦¬ìŠ¤íŠ¸ (list)
        - dagster/row_count: total_postsì™€ ë™ì¼ (int)
    Raises:
        Exception: ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ
    """

    # ë””ë ‰í† ë¦¬ ê¸°ë°˜ Crawlee JS ì‹¤í–‰ ë° ë¡œê·¸ ê¸°ë°˜ í†µê³„ ìˆ˜ì§‘
    context.log.info("ğŸ”„ IS í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° ìˆœì°¨ ì‹¤í–‰ ì‹œì‘")

    base_path = Path("/Users/craigchoi/silla/dag/dag/is_crawlee")
    if not base_path.exists():
        base_path = Path("./dag/is_crawlee")
    crawler_dirs = sorted([d for d in base_path.iterdir() if d.is_dir()], key=lambda d: d.name)
    if not crawler_dirs:
        context.log.warning("âš ï¸ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return dg.MaterializeResult(metadata={"total_crawlers": 0, "dagster/row_count": 0})
    context.log.info(f"ğŸ” ì‹¤í–‰ ëŒ€ìƒ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬: {[d.name for d in crawler_dirs]}")

    per_crawler_stats = []

    try:
        for crawler_dir in crawler_dirs:
            context.log.info(f"â–¶ '{crawler_dir.name}' í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘")
            start_time = datetime.now()
            command = ["node", "dist/main.js"]
            result = subprocess.run(
                command,
                cwd=str(crawler_dir),
                capture_output=True,
                text=True,
                timeout=1800,
                env={**os.environ, "NODE_ENV": "production", "NO_COLOR": "1"}
            )
            duration = (datetime.now() - start_time).total_seconds()
            clean_stdout = clean_ansi_codes(result.stdout)
            clean_stderr = clean_ansi_codes(result.stderr)
            stats = extract_crawl_stats(clean_stdout)
            collected = stats.get("posts_collected", 0)
            context.log.info(f"ğŸ“¥ '{crawler_dir.name}' í¬ë¡¤ëŸ¬ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜: {collected}ê°œ")
            snippet_stdout = clean_stdout[-500:] if clean_stdout else ""
            snippet_stderr = clean_stderr[-500:] if clean_stderr else ""
            finished_ok = (stats.get("failed") == 0 and (stats.get("total_requests") or 0) > 0 and (stats.get("succeeded") or 0) >= (stats.get("total_requests") or 0))
            terminal = bool(stats.get("terminal")) if "terminal" in stats else None
            per_crawler_stats.append({
                "name": crawler_dir.name,
                "status": "success" if result.returncode == 0 or finished_ok else "failure",
                "terminal": terminal,
                "requests": stats.get("total_requests"),
                "succeeded": stats.get("succeeded"),
                "failed": stats.get("failed"),
                "posts_collected": stats.get("posts_collected"),
                "embedded_total": stats.get("embedded_total"),
                "embedded_by_mime": stats.get("embedded_by_mime"),
                "duration_seconds": duration,
                "log_stdout_snippet": snippet_stdout,
                "log_stderr_snippet": snippet_stderr
            })
            if result.returncode != 0 and not finished_ok:
                raise CrawlerExecutionError(f"{crawler_dir.name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {snippet_stderr or snippet_stdout}")

        total_requests = sum(item["requests"] or 0 for item in per_crawler_stats)
        total_succeeded = sum(item["succeeded"] or 0 for item in per_crawler_stats)
        total_failed = sum(item["failed"] or 0 for item in per_crawler_stats)
        total_posts = sum(item["posts_collected"] or 0 for item in per_crawler_stats)
        # ì„ë² ë””ë“œ mimeType ì§‘ê³„
        from collections import Counter
        embedded_counter = Counter()
        for item in per_crawler_stats:
            for k, v in (item.get("embedded_by_mime") or {}).items():
                embedded_counter[k] += v
        total_embedded = sum(embedded_counter.values())
        total_duration = sum(item["duration_seconds"] or 0 for item in per_crawler_stats)
        avg_requests_per_second = total_requests / total_duration if total_duration > 0 else 0
        avg_posts_per_second = total_posts / total_duration if total_duration > 0 else 0
        posts_collected = total_posts

        metadata = {
            "total_crawlers": len(per_crawler_stats),
            "total_requests": total_requests,
            "total_succeeded": total_succeeded,
            "total_failed": total_failed,
            "posts_collected": posts_collected,
            "average_requests_per_second": round(avg_requests_per_second, 2),
            "average_posts_per_second": round(avg_posts_per_second, 2),
            "embedded_total": total_embedded,
            "embedded_by_mime": dict(embedded_counter),
            "per_crawler_stats": per_crawler_stats,
            "dagster/row_count": posts_collected
        }

        context.log.info(f"âœ… ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ: {len(per_crawler_stats)}ê°œ í¬ë¡¤ëŸ¬, ì´ ìˆ˜ì§‘ ê²Œì‹œë¬¼ {posts_collected}ê°œ")
        return dg.MaterializeResult(metadata=metadata)

    except subprocess.TimeoutExpired as e:
        context.log.error("â° í¬ë¡¤ëŸ¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
        raw_out = getattr(e, "output", None) or getattr(e, "stdout", None) or b""
        raw_err = getattr(e, "stderr", None) or b""
        snippet_stdout = clean_ansi_codes(raw_out)
        snippet_stderr = clean_ansi_codes(raw_err)
        per_crawler_stats.append({
            "name": crawler_dir.name if 'crawler_dir' in locals() else None,
            "status": "timeout",
            "log_stdout_snippet": snippet_stdout,
            "log_stderr_snippet": snippet_stderr
        })
        # Fast-fail: surface timeout as a Dagster failure
        raise dg.Failure(
            description="30ë¶„ íƒ€ì„ì•„ì›ƒ ë°œìƒ",
            metadata={
                "total_crawlers": len(per_crawler_stats),
                "per_crawler_stats": per_crawler_stats,
                "error_type": "timeout",
                "error_message": "30ë¶„ íƒ€ì„ì•„ì›ƒ ë°œìƒ",
                "dagster/row_count": sum(item.get("posts_collected") or 0 for item in per_crawler_stats)
            }
        )

    except Exception as e:
        context.log.error(f"ğŸ’¥ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        # Fast-fail: bubble up as a Dagster failure so the run is marked failed
        raise dg.Failure(
            description="í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ",
            metadata={
                "total_crawlers": len(per_crawler_stats),
                "per_crawler_stats": per_crawler_stats,
                "error_type": "exception",
                "error_message": str(e)[:500],
                "dagster/row_count": sum(item.get("posts_collected") or 0 for item in per_crawler_stats)
            }
        )

@schedule(
    # (30m=8/38, 1h=:18, 3h=:28, 6h=:48, 12h=0ë¶„ 12ì‹œê°„ ë‹¨ìœ„, 24h=2:58/14:58, 48h=4:50 */2)
    cron_schedule=[
        "8,38 * * * *",    # every 30 minutes
        "18 * * * *",      # every hour
        "28 */3 * * *",    # every 3 hours
        "48 */6 * * *",    # every 6 hours
        "0 */12 * * *",    # every 12 hours
        "58 2,14 * * *",   # every 24 hours
        "50 4 */2 * *",     # every 48 hours
    ],
    job=is_crawler_parallel_job,
    execution_timezone="Asia/Seoul",
)
def is_crawler_executor_interval_schedule(context):
    ts = context.scheduled_execution_time
    minute, hour, day = ts.minute, ts.hour, ts.day

    if minute in (8, 38):
        flag, value = "--minutes", 30
    elif minute == 50 and hour == 4:
        # 48h: 04:50 every 2 days
        flag, value = "--hours", 48
    elif minute == 58 and hour in (2, 14):
        # 24h: 02:58 and 14:58 daily
        flag, value = "--hours", 24
    elif minute == 0 and (hour % 12 == 0):
        # 12h: at 00:00 and 12:00
        flag, value = "--hours", 12
    elif minute == 48 and (hour % 6 == 0):
        # 6h windows
        flag, value = "--hours", 6
    elif minute == 28 and (hour % 3 == 0):
        # 3h windows
        flag, value = "--hours", 3
    elif minute == 18:
        # 1h windows
        flag, value = "--hours", 1
    else:
        # Fallback (should not normally trigger if cron list is authoritative)
        flag, value = "--hours", 1

    return RunRequest(
        run_key=f"{flag}_{value}_{ts.strftime('%Y%m%dT%H%M')}",
        run_config={
            "ops": {
                "run_crawler": {
                    "config": {
                        "args": [flag, str(value)]
                    }
                }
            },
            "execution": {
                "config": {
                    "multiprocess": {
                        "max_concurrent": 4
                    }
                }
            }
        },
        tags={"interval": f"{value}{flag}"}
    )


@dg.asset(
    config_schema={"args": list},
    deps=[is_crawler_build],
    group_name="IS",
    kinds={"source"},
    tags={
        "domain": "community_content",
        "data_tier": "bronze",
        "source": "node_script",
        "technology": "crawlee",
        "framework": "playwright",
        "extraction_type": "web_scraping"
    },
    description="ìŠ¤ì¼€ì¤„ ì „ë‹¬ ì¸ìì— ë”°ë¼ Crawlee JSë¥¼ ì‹¤í–‰í•˜ê³ , ë¡œê·¸ í†µê³„ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
)
def is_crawler_executor_interval(
    context: dg.AssetExecutionContext
) -> dg.MaterializeResult:
    """
    Interval-based Crawlee JS ì‹¤í–‰ Asset.

    ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ì „ë‹¬ëœ `args` íŒŒë¼ë¯¸í„°(ì˜ˆ: `--minute 30`, `--hour 3`)ë¥¼ ì‚¬ìš©í•˜ì—¬,
    ê° í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ë‚´ `node dist/main.js`ë¥¼ ì‹¤í–‰í•˜ê³ , ë¡œê·¸ì—ì„œ í†µê³„ë¥¼ ì¶”ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        context: Dagster AssetExecutionContext; context.op_config["args"]ì— ì‹¤í–‰ ì¸ì ë¦¬ìŠ¤íŠ¸ê°€ í¬í•¨ë©ë‹ˆë‹¤.

    Returns:
        dg.MaterializeResult: ì‹¤í–‰ í†µê³„ ë° ë©”íƒ€ë°ì´í„° í¬í•¨ ê²°ê³¼.
    """

    # ë””ë ‰í† ë¦¬ ê¸°ë°˜ Crawlee JS ì‹¤í–‰ ë° ë¡œê·¸ ê¸°ë°˜ í†µê³„ ìˆ˜ì§‘
    context.log.info("ğŸ”„ IS í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° ìˆœì°¨ ì‹¤í–‰ ì‹œì‘")

    base_path = Path("/Users/craigchoi/silla/dag/dag/is_crawlee")
    if not base_path.exists():
        base_path = Path("./dag/is_crawlee")
    crawler_dirs = sorted([d for d in base_path.iterdir() if d.is_dir()], key=lambda d: d.name)
    if not crawler_dirs:
        context.log.warning("âš ï¸ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return dg.MaterializeResult(metadata={"total_crawlers": 0, "dagster/row_count": 0})
    context.log.info(f"ğŸ” ì‹¤í–‰ ëŒ€ìƒ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬: {[d.name for d in crawler_dirs]}")

    per_crawler_stats = []

    try:
        for crawler_dir in crawler_dirs:
            context.log.info(f"â–¶ '{crawler_dir.name}' í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘")
            start_time = datetime.now()
            # ì•ˆì „í•˜ê²Œ ìŠ¤ì¼€ì¤„ ì¸ìë¥¼ ê°€ì ¸ì™€ node ì‹¤í–‰ ì»¤ë§¨ë“œì— ì¶”ê°€
            args = context.op_config.get("args") or []
            # ëª¨ë“  ì¸ìë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ëª…ë ¹ì–´ì— ë³‘í•©
            cmd_args = ["node", "dist/main.js", "--mode", "full"] + [str(arg) for arg in args]
            context.log.info(f"â–¶ ì‹¤í–‰ ì»¤ë§¨ë“œ: {cmd_args}")
            result = subprocess.run(
                cmd_args,
                cwd=str(crawler_dir),
                capture_output=True,
                text=True,
                timeout=1800,
                env={**os.environ, "NODE_ENV": "production", "NO_COLOR": "1"}
            )
            duration = (datetime.now() - start_time).total_seconds()
            clean_stdout = clean_ansi_codes(result.stdout)
            clean_stderr = clean_ansi_codes(result.stderr)
            stats = extract_crawl_stats(clean_stdout)
            collected = stats.get("posts_collected", 0)
            context.log.info(f"ğŸ“¥ '{crawler_dir.name}' í¬ë¡¤ëŸ¬ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜: {collected}ê°œ")
            snippet_stdout = clean_stdout[-500:] if clean_stdout else ""
            snippet_stderr = clean_stderr[-500:] if clean_stderr else ""
            finished_ok = (stats.get("failed") == 0 and (stats.get("total_requests") or 0) > 0 and (stats.get("succeeded") or 0) >= (stats.get("total_requests") or 0))
            terminal = bool(stats.get("terminal")) if "terminal" in stats else None
            per_crawler_stats.append({
                "name": crawler_dir.name,
                "status": "success" if result.returncode == 0 or finished_ok else "failure",
                "terminal": terminal,
                "requests": stats.get("total_requests"),
                "succeeded": stats.get("succeeded"),
                "failed": stats.get("failed"),
                "posts_collected": stats.get("posts_collected"),
                "embedded_total": stats.get("embedded_total"),
                "embedded_by_mime": stats.get("embedded_by_mime"),
                "duration_seconds": duration,
                "log_stdout_snippet": snippet_stdout,
                "log_stderr_snippet": snippet_stderr
            })
            if result.returncode != 0 and not finished_ok:
                raise CrawlerExecutionError(f"{crawler_dir.name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {snippet_stderr or snippet_stdout}")

        total_requests = sum(item["requests"] or 0 for item in per_crawler_stats)
        total_succeeded = sum(item["succeeded"] or 0 for item in per_crawler_stats)
        total_failed = sum(item["failed"] or 0 for item in per_crawler_stats)
        total_posts = sum(item["posts_collected"] or 0 for item in per_crawler_stats)
        # ì„ë² ë””ë“œ mimeType ì§‘ê³„
        from collections import Counter
        embedded_counter = Counter()
        for item in per_crawler_stats:
            for k, v in (item.get("embedded_by_mime") or {}).items():
                embedded_counter[k] += v
        total_embedded = sum(embedded_counter.values())
        total_duration = sum(item["duration_seconds"] or 0 for item in per_crawler_stats)
        avg_requests_per_second = total_requests / total_duration if total_duration > 0 else 0
        avg_posts_per_second = total_posts / total_duration if total_duration > 0 else 0
        posts_collected = total_posts

        metadata = {
            "total_crawlers": len(per_crawler_stats),
            "total_requests": total_requests,
            "total_succeeded": total_succeeded,
            "total_failed": total_failed,
            "posts_collected": posts_collected,
            "average_requests_per_second": round(avg_requests_per_second, 2),
            "average_posts_per_second": round(avg_posts_per_second, 2),
            "embedded_total": total_embedded,
            "embedded_by_mime": dict(embedded_counter),
            "per_crawler_stats": per_crawler_stats,
            "dagster/row_count": posts_collected
        }

        context.log.info(f"âœ… ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ: {len(per_crawler_stats)}ê°œ í¬ë¡¤ëŸ¬, ì´ ìˆ˜ì§‘ ê²Œì‹œë¬¼ {posts_collected}ê°œ")
        return dg.MaterializeResult(metadata=metadata)

    except subprocess.TimeoutExpired as e:
        context.log.error("â° í¬ë¡¤ëŸ¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
        raw_out = getattr(e, "output", None) or getattr(e, "stdout", None) or b""
        raw_err = getattr(e, "stderr", None) or b""
        snippet_stdout = clean_ansi_codes(raw_out)
        snippet_stderr = clean_ansi_codes(raw_err)
        per_crawler_stats.append({
            "name": crawler_dir.name if 'crawler_dir' in locals() else None,
            "status": "timeout",
            "log_stdout_snippet": snippet_stdout,
            "log_stderr_snippet": snippet_stderr
        })
        # Fast-fail: surface timeout as a Dagster failure
        raise dg.Failure(
            description="30ë¶„ íƒ€ì„ì•„ì›ƒ ë°œìƒ",
            metadata={
                "total_crawlers": len(per_crawler_stats),
                "per_crawler_stats": per_crawler_stats,
                "error_type": "timeout",
                "error_message": "30ë¶„ íƒ€ì„ì•„ì›ƒ ë°œìƒ",
                "dagster/row_count": sum(item.get("posts_collected") or 0 for item in per_crawler_stats)
            }
        )

    except Exception as e:
        context.log.error(f"ğŸ’¥ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        # Fast-fail: bubble up as a Dagster failure so the run is marked failed
        raise dg.Failure(
            description="í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ",
            metadata={
                "total_crawlers": len(per_crawler_stats),
                "per_crawler_stats": per_crawler_stats,
                "error_type": "exception",
                "error_message": str(e)[:500],
                "dagster/row_count": sum(item.get("posts_collected") or 0 for item in per_crawler_stats)
            }
        )
