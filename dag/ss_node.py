import json
import os
import re
import subprocess
from datetime import datetime
from pathlib import Path

import dagster as dg

from dagster import op

# í¬ë¡¤ëŸ¬ ë¹Œë“œ ë˜ëŠ” ì‹¤í–‰ ì¤‘ì— ë°œìƒí•˜ëŠ” ì˜ˆì™¸ì…ë‹ˆë‹¤.
class CrawlerExecutionError(Exception):
    """í¬ë¡¤ëŸ¬ ë¹Œë“œ ë˜ëŠ” ì‹¤í–‰ ì¤‘ì— ë°œìƒí•˜ëŠ” ì˜ˆì™¸ì…ë‹ˆë‹¤."""
    pass


def extract_crawl_stats(log_text: str) -> dict[str, int]:
    """
    í¬ë¡¤ë§ í†µê³„ ì •ë³´ë¥¼ ë¡œê·¸ì—ì„œ ì¶”ì¶œ
    Crawlee ë¡œê·¸ì—ì„œ ìš”ì²­ í†µê³„ì™€ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ íšŸìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì§‘ ê±´ìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë˜í•œ ë¡œê·¸ ë‚´ ì„ë² ë””ë“œ(embedded) í•­ëª©ì˜ mimeType ë¹ˆë„ë¥¼ ì§‘ê³„í•©ë‹ˆë‹¤.
    """
    stats = {
        "total_requests": 0,
        "succeeded": 0,
        "failed": 0,
        "posts_collected": 0,
        "embedded_total": 0,
        "embedded_by_mime": {},
        "terminal": None,
    }
    # Also parse the finishing summary line to reduce false negatives
    finished_match = re.search(r'Finished!\s+Total\s+(\d+)\s+requests:\s+(\d+)\s+succeeded,\s+(\d+)\s+failed\.\s*(\{.*?\})?', log_text)
    if finished_match:
        try:
            total = int(finished_match.group(1))
            succ = int(finished_match.group(2))
            fail = int(finished_match.group(3))
            # Prefer explicit finished numbers when present
            stats["total_requests"] = max(stats.get("total_requests", 0), total)
            stats["succeeded"] = succ
            stats["failed"] = fail
            # Parse optional trailing JSON for terminal flag
            tail = finished_match.group(4)
            if tail:
                try:
                    tail_json = json.loads(tail)
                    stats["terminal"] = bool(tail_json.get("terminal"))
                except Exception:
                    stats["terminal"] = None
            else:
                stats["terminal"] = None
        except Exception:
            pass

    # Final statistics JSON íŒŒì‹±
    final_pattern = r'Final request statistics: (\{.*?\})'
    final_match = re.search(final_pattern, log_text, re.DOTALL)
    if final_match:
        try:
            stats_json = json.loads(final_match.group(1))
            stats["total_requests"] = stats_json.get("requestsFinished", 0)
            stats["succeeded"] = stats_json.get("requestsFinished", 0) - stats_json.get("requestsFailed", 0)
            stats["failed"] = stats_json.get("requestsFailed", 0)
        except Exception:
            # í†µê³„ JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¬´ì‹œí•˜ê³  ì§„í–‰
            pass

    # ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ íšŸìˆ˜ ì¹´ìš´íŠ¸
    detail_pattern = r'ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬'
    stats["posts_collected"] = len(re.findall(detail_pattern, log_text))

    # embedded mimeType ì¹´ìš´íŠ¸: JSON/ë¡œê·¸ í˜•íƒœ ëª¨ë‘ í—ˆìš© (í‚¤/ê°’ ìˆœì„œ ë¶ˆë¬¸, ì‘ì€ë”°ì˜´í‘œ/í°ë”°ì˜´í‘œ í˜¼ìš© í—ˆìš©)
    # ì˜ˆì‹œ ë§¤ì¹˜ ëŒ€ìƒ:
    #  - { "embedded": [{ "mimeType": "image/png", ... }] }
    #  - mimeType: 'application/pdf'
    #  - mime_type = "text/html"
    mime_regex = re.compile(r"(?:\bmimeType\b|\bmime_type\b)\s*[:=]\s*['\"]?([\w.-]+\/[\w.+-]+)", re.IGNORECASE)
    mime_counts: dict[str, int] = {}
    for match in mime_regex.finditer(log_text):
        mime = match.group(1).lower()
        mime_counts[mime] = mime_counts.get(mime, 0) + 1

    stats["embedded_by_mime"] = mime_counts
    stats["embedded_total"] = sum(mime_counts.values())

    return stats


def clean_ansi_codes(text: str | bytes) -> str:
    """
    ANSI ì»¬ëŸ¬ ì½”ë“œë¥¼ ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë¡œê·¸ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Crawleeì™€ Playwrightì—ì„œ ì¶œë ¥ë˜ëŠ” ì»¬ëŸ¬ ì½”ë“œë¥¼ ì œê±°í•˜ì—¬
    Dagster ë¡œê·¸ì—ì„œ ì½ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.

    Args:
        text: ANSI ì½”ë“œê°€ í¬í•¨ëœ ì›ë³¸ í…ìŠ¤íŠ¸ (str ë˜ëŠ” bytes)

    Returns:
        ANSI ì½”ë“œê°€ ì œê±°ëœ ê¹”ë”í•œ í…ìŠ¤íŠ¸
    """
    if isinstance(text, (bytes, bytearray)):
        try:
            text = text.decode("utf-8", errors="ignore")
        except Exception:
            # í˜¹ì‹œ ëª¨ë¥¼ ë””ì½”ë”© ì´ìŠˆì— ëŒ€ë¹„
            text = str(text)
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    return ansi_escape.sub('', text)

BASE_CRAWLER_PATH = Path("/Users/craigchoi/silla/dag/dag/ss_crawlee")

def list_crawler_dirs(base_path: Path) -> list[Path]:
    """Return sorted immediate child directories under base_path."""
    return sorted([d for d in base_path.iterdir() if d.is_dir()], key=lambda d: d.name)

@op
def build_crawlers(context: dg.OpExecutionContext) -> int:
    """Build all Crawlee projects (op version of ss_crawler_build).
    Returns the number of crawler directories built. This op exists so that
    the parallel run job can depend on a build step within the same run.
    """
    context.log.info("ğŸ”¨ [build_crawlers] ì‹œì‘")
    base_path = BASE_CRAWLER_PATH if BASE_CRAWLER_PATH.exists() else Path("./dag/ss_crawlee")
    crawler_dirs = list_crawler_dirs(base_path)
    if not crawler_dirs:
        context.log.warning(f"âš ï¸ ë¹Œë“œ ëŒ€ìƒ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {base_path}")
        return 0

    build_command = ["npm", "run", "build"]
    timeout_seconds = 300

    for crawler_dir in crawler_dirs:
        context.log.info(f"ğŸ”¨ ë¹Œë“œ: {crawler_dir.name}")
        result = subprocess.run(
            build_command,
            cwd=str(crawler_dir),
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            env={**os.environ, "NODE_ENV": "production", "NO_COLOR": "1"}
        )
        if result.returncode != 0:
            context.log.error(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {crawler_dir.name} (ì½”ë“œ: {result.returncode})")
            context.log.error(result.stdout)
            context.log.error(result.stderr)
            raise CrawlerExecutionError(f"{crawler_dir.name} ë¹Œë“œ ì‹¤íŒ¨: {result.stderr or result.stdout}")
    context.log.info(f"âœ… ë¹Œë“œ ì™„ë£Œ: {len(crawler_dirs)}ê°œ")
    return len(crawler_dirs)

@dg.asset(
    group_name="SS",
    kinds={"build"},
    tags={
        "domain": "community_content",
        "process": "build",
        "technology": "typescript",
        "framework": "crawlee",
        "tier": "infrastructure"
    },
    description="SS í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  Crawlee TypeScript ì½”ë“œë¥¼ JavaScriptë¡œ ë¹Œë“œí•©ë‹ˆë‹¤."
)
def ss_crawler_build(
    context: dg.AssetExecutionContext
) -> dg.MaterializeResult:
    """
    SS í¬ë¡¤ëŸ¬ ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  Crawlee í”„ë¡œì íŠ¸ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤.

    ì´ Assetì€ ì§€ì •ëœ ìƒìœ„ í´ë”ë¥¼ ë‹¨ì¼ ë ˆë²¨ë¡œ ìŠ¤ìº”í•˜ì—¬, ê° í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ì—ì„œ
    `npm run build` ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ JavaScript íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ë¹Œë“œ ê³¼ì •:
    1. ìµœìƒìœ„ í•œ ë ˆë²¨ì˜ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ëª©ë¡ ìˆ˜ì§‘
    2. ê° ë””ë ‰í† ë¦¬ì—ì„œ `npm run build` ì‹¤í–‰ (5ë¶„ íƒ€ì„ì•„ì›ƒ)
    3. ëª¨ë“  í¬ë¡¤ëŸ¬ ë¹Œë“œ ì™„ë£Œ ì—¬ë¶€ í™•ì¸
    4. ë¹Œë“œëœ í¬ë¡¤ëŸ¬ ìˆ˜ë¥¼ `dagster/row_count` ë©”íƒ€ë°ì´í„°ë¡œ ë°˜í™˜

    Returns:
        MaterializeResult: ë¹Œë“œ ë©”íƒ€ë°ì´í„°
            - build_success: ì „ì²´ ë¹Œë“œ ì„±ê³µ ì—¬ë¶€ (bool)
            - dagster/row_count: ë¹Œë“œëœ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìˆ˜ (int)

    Raises:
        CrawlerExecutionError: ë¹Œë“œ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ
    """

    context.log.info("ğŸ”¨ SS í¬ë¡¤ëŸ¬ ë¹Œë“œ ì‹œì‘ - ìƒìœ„ ë‹¨ì¼ ë ˆë²¨ ìŠ¤ìº”")
    build_start_time = datetime.now()

    # ë¹Œë“œí•  í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìƒìœ„ ê²½ë¡œ (ë‹¨ì¼ ë ˆë²¨ ìŠ¤ìº”)
    base_path = BASE_CRAWLER_PATH if BASE_CRAWLER_PATH.exists() else Path("./dag/ss_crawlee")
    context.log.info(f"ğŸ” í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ íƒìƒ‰ ê²½ë¡œ: {base_path}")

    crawler_dirs = list_crawler_dirs(base_path)
    context.log.info(f"ğŸ” ë°œê²¬ëœ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬: {[d.name for d in crawler_dirs]}")

    build_command = ["npm", "run", "build"]
    timeout_seconds = 300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ

    for crawler_dir in crawler_dirs:
        context.log.info(f"ğŸ”¨ ë¹Œë“œ ì‹œì‘: {crawler_dir.name}")
        result = subprocess.run(
            build_command,
            cwd=str(crawler_dir),
            capture_output=True,
            text=True,
            timeout=timeout_seconds,
            env={**os.environ, "NODE_ENV": "production", "NO_COLOR": "1"}
        )
        if result.returncode != 0:
            context.log.error(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {crawler_dir.name} (ì½”ë“œ: {result.returncode})")
            context.log.error(result.stdout)
            context.log.error(result.stderr)
            raise CrawlerExecutionError(f"{crawler_dir.name} ë¹Œë“œ ì‹¤íŒ¨: {result.stderr or result.stdout}")

    build_duration = (datetime.now() - build_start_time).total_seconds()
    built_count = len(crawler_dirs)
    context.log.info(f"âœ… ëª¨ë“  í¬ë¡¤ëŸ¬ ë¹Œë“œ ì„±ê³µ! ë¹Œë“œ ê°œìˆ˜: {built_count}, ì†Œìš” ì‹œê°„: {build_duration:.2f}ì´ˆ")

    return dg.MaterializeResult(
        metadata={
            "build_success": True,
            "dagster/row_count": built_count
        }
    )

@dg.asset(
    deps=[ss_crawler_build],  # ë¹Œë“œ Assetì— ì˜ì¡´
    group_name="SS",
    kinds={"source"},
    tags={
        "domain": "community_content",
        "data_tier": "bronze",
        "source": "node_script",
        "technology": "crawlee",
        "framework": "playwright",
        "extraction_type": "web_scraping"
    },
    description="SS í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  Crawlee ë¹Œë“œëœ JSë¥¼ ìˆœì°¨ ì‹¤í–‰í•˜ê³ , ë¡œê·¸ ê¸°ë°˜ í†µê³„ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
)
def ss_crawler_executor(
    context: dg.AssetExecutionContext
) -> dg.MaterializeResult:
    """
    ë””ë ‰í† ë¦¬ ê¸°ë°˜ Crawlee JS ì‹¤í–‰ ë° ë¡œê·¸ ê¸°ë°˜ í†µê³„ ìˆ˜ì§‘ Asset.

    ì´ Assetì€ ë¹Œë“œëœ Crawlee JavaScript íŒŒì¼ì´ ìˆëŠ” ê° í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ë¥¼ ë‹¨ì¼ ë ˆë²¨ë¡œ ìŠ¤ìº”í•˜ì—¬,
    ìˆœì°¨ì ìœ¼ë¡œ `node dist/main.js`ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ì‹¤í–‰ í›„ ë¡œê·¸ì˜ ìµœì¢… ìš”ì²­ í†µê³„ì™€
    ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ íšŸìˆ˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ë©”íƒ€ë°ì´í„°ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì‹¤í–‰ ê³¼ì •:
    1. ìƒìœ„ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ë‹¨ì¼ ë ˆë²¨ ìŠ¤ìº”
    2. ê° ë””ë ‰í† ë¦¬ë³„ `node dist/main.js` ì‹¤í–‰ (30ë¶„ íƒ€ì„ì•„ì›ƒ)
    3. Crawlee ë¡œê·¸ì˜ "Final request statistics" JSON íŒŒì‹±:
       - requestsFinished â†’ total_requests
       - requestsFailed â†’ failed, succeeded = total_requests - failed
    4. ë¡œê·¸ì—ì„œ "ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬" ë°œìƒ íšŸìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•˜ì—¬ posts_collectedë¡œ ì„¤ì •
    5. í¬ë¡¤ëŸ¬ë³„ ë° ì „ì²´ í†µê³„ ì§‘ê³„ í›„ ë°˜í™˜

    Returns:
        MaterializeResult: ì‹¤í–‰ í†µê³„ ë° í¬ë¡¤ëŸ¬ë³„ ìƒì„¸ ì •ë³´ í¬í•¨ ë©”íƒ€ë°ì´í„°
        - total_crawlers: ì‹¤í–‰ëœ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìˆ˜ (int)
        - total_requests: ì´ ìš”ì²­ ìˆ˜ (int)
        - succeeded: ì„±ê³µ ìš”ì²­ ìˆ˜ (int)
        - failed: ì‹¤íŒ¨ ìš”ì²­ ìˆ˜ (int)
        - posts_collected: ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ê±´ìˆ˜ í•©ê³„ (int)
        - per_crawler_stats: ê°œë³„ í¬ë¡¤ëŸ¬ë³„ í†µê³„ ë¦¬ìŠ¤íŠ¸ (list)
        - dagster/row_count: total_postsì™€ ë™ì¼ (int)
    Raises:
        Exception: ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ
    """

    # ë””ë ‰í† ë¦¬ ê¸°ë°˜ Crawlee JS ì‹¤í–‰ ë° ë¡œê·¸ ê¸°ë°˜ í†µê³„ ìˆ˜ì§‘
    context.log.info("ğŸ”„ SS í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° ìˆœì°¨ ì‹¤í–‰ ì‹œì‘")

    base_path = BASE_CRAWLER_PATH if BASE_CRAWLER_PATH.exists() else Path("./dag/ss_crawlee")
    crawler_dirs = list_crawler_dirs(base_path)
    if not crawler_dirs:
        context.log.warning("âš ï¸ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return dg.MaterializeResult(metadata={"total_crawlers": 0, "dagster/row_count": 0})
    context.log.info(f"ğŸ” ì‹¤í–‰ ëŒ€ìƒ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬: {[d.name for d in crawler_dirs]}")

    per_crawler_stats = []

    try:
        for crawler_dir in crawler_dirs:
            context.log.info(f"â–¶ '{crawler_dir.name}' í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘")
            start_time = datetime.now()
            command = ["node", "dist/main.js", "--mode", "full", "--hours", "48"]
            result = subprocess.run(
                command,
                cwd=str(crawler_dir),
                capture_output=True,
                text=True,
                timeout=1800,
                env={**os.environ, "NODE_ENV": "production", "NO_COLOR": "1"}
            )
            duration = (datetime.now() - start_time).total_seconds()
            clean_stdout = clean_ansi_codes(result.stdout)
            clean_stderr = clean_ansi_codes(result.stderr)
            stats = extract_crawl_stats(clean_stdout)
            collected = stats.get("posts_collected", 0)
            context.log.info(f"ğŸ“¥ '{crawler_dir.name}' í¬ë¡¤ëŸ¬ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ìˆ˜: {collected}ê°œ")
            snippet_stdout = clean_stdout[-500:] if clean_stdout else ""
            snippet_stderr = clean_stderr[-500:] if clean_stderr else ""
            finished_ok = (stats.get("failed") == 0 and (stats.get("total_requests") or 0) > 0 and (stats.get("succeeded") or 0) >= (stats.get("total_requests") or 0))
            terminal = bool(stats.get("terminal")) if "terminal" in stats else None
            per_crawler_stats.append({
                "name": crawler_dir.name,
                "status": "success" if result.returncode == 0 or finished_ok else "failure",
                "terminal": terminal,
                "requests": stats.get("total_requests"),
                "succeeded": stats.get("succeeded"),
                "failed": stats.get("failed"),
                "posts_collected": stats.get("posts_collected"),
                "embedded_total": stats.get("embedded_total"),
                "embedded_by_mime": stats.get("embedded_by_mime"),
                "duration_seconds": duration,
                "log_stdout_snippet": snippet_stdout,
                "log_stderr_snippet": snippet_stderr
            })
            if result.returncode != 0 and not finished_ok:
                raise CrawlerExecutionError(f"{crawler_dir.name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {snippet_stderr or snippet_stdout}")

        total_requests = sum(item["requests"] or 0 for item in per_crawler_stats)
        total_succeeded = sum(item["succeeded"] or 0 for item in per_crawler_stats)
        total_failed = sum(item["failed"] or 0 for item in per_crawler_stats)
        total_posts = sum(item["posts_collected"] or 0 for item in per_crawler_stats)
        # ì„ë² ë””ë“œ mimeType ì§‘ê³„
        from collections import Counter
        embedded_counter = Counter()
        for item in per_crawler_stats:
            for k, v in (item.get("embedded_by_mime") or {}).items():
                embedded_counter[k] += v
        total_embedded = sum(embedded_counter.values())
        total_duration = sum(item["duration_seconds"] or 0 for item in per_crawler_stats)
        avg_requests_per_second = total_requests / total_duration if total_duration > 0 else 0
        avg_posts_per_second = total_posts / total_duration if total_duration > 0 else 0
        posts_collected = total_posts

        metadata = {
            "total_crawlers": len(per_crawler_stats),
            "total_requests": total_requests,
            "total_succeeded": total_succeeded,
            "total_failed": total_failed,
            "posts_collected": posts_collected,
            "average_requests_per_second": round(avg_requests_per_second, 2),
            "average_posts_per_second": round(avg_posts_per_second, 2),
            "embedded_total": total_embedded,
            "embedded_by_mime": dict(embedded_counter),
            "per_crawler_stats": per_crawler_stats,
            "dagster/row_count": posts_collected
        }

        context.log.info(f"âœ… ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ: {len(per_crawler_stats)}ê°œ í¬ë¡¤ëŸ¬, ì´ ìˆ˜ì§‘ ê²Œì‹œë¬¼ {posts_collected}ê°œ")
        return dg.MaterializeResult(metadata=metadata)

    except subprocess.TimeoutExpired as e:
        context.log.error("â° í¬ë¡¤ëŸ¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ë°œìƒ")
        raw_out = getattr(e, "output", None) or getattr(e, "stdout", None) or b""
        raw_err = getattr(e, "stderr", None) or b""
        snippet_stdout = clean_ansi_codes(raw_out)
        snippet_stderr = clean_ansi_codes(raw_err)
        per_crawler_stats.append({
            "name": crawler_dir.name if 'crawler_dir' in locals() else None,
            "status": "timeout",
            "log_stdout_snippet": snippet_stdout,
            "log_stderr_snippet": snippet_stderr
        })
        # Fast-fail: surface timeout as a Dagster failure
        raise dg.Failure(
            description="30ë¶„ íƒ€ì„ì•„ì›ƒ ë°œìƒ",
            metadata={
                "total_crawlers": len(per_crawler_stats),
                "per_crawler_stats": per_crawler_stats,
                "error_type": "timeout",
                "error_message": "30ë¶„ íƒ€ì„ì•„ì›ƒ ë°œìƒ",
                "dagster/row_count": sum(item.get("posts_collected") or 0 for item in per_crawler_stats)
            }
        )

    except Exception as e:
        context.log.error(f"ğŸ’¥ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        # Fast-fail: bubble up as a Dagster failure so the run is marked failed
        raise dg.Failure(
            description="í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ",
            metadata={
                "total_crawlers": len(per_crawler_stats),
                "per_crawler_stats": per_crawler_stats,
                "error_type": "exception",
                "error_message": str(e)[:500],
                "dagster/row_count": sum(item.get("posts_collected") or 0 for item in per_crawler_stats)
            }
        )
