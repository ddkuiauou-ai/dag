"""
NPS Raw Data Ingestion - Bronze Tier
êµ­ë¯¼ì—°ê¸ˆ ì›ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ìˆ˜ì§‘
"""

from dataclasses import dataclass, field
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
import re
import threading
import concurrent.futures
import logging
from typing import Any
from contextlib import contextmanager
import time
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from dagster import AssetExecutionContext, asset
from dagster_duckdb import DuckDBResource
import pandas as pd
import shutil
import dagster as dg
import chardet
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import traceback

# =============================================================================
# 1ë‹¨ê³„: ì „ì—­ ìƒìˆ˜ ë° ì„¤ì •
# =============================================================================

@dataclass
class NPSConfig:
    """NPS ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •"""
    # ë‹¤ìš´ë¡œë“œ ì„¤ì •
    max_concurrent_downloads: int = 3
    request_timeout: int = 10
    retry_attempts: int = 3
    retry_backoff_factor: float = 1.0
    
    # íŒŒì¼ ì‹œìŠ¤í…œ ì„¤ì •
    required_disk_space_gb: float = 10.0
    min_file_size_kb: int = 100
    batch_size: int = 1000
    
    # ë‚ ì§œ ë²”ìœ„ ê²€ì¦
    min_year: int = 2015
    max_year: int = 2025
    
    # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì„¤ì •
    skip_label: str = "..."

@dataclass
class NPSFilePaths:
    """NPS íŒŒì¼ ê²½ë¡œ ê´€ë¦¬"""
    base_dir: Path = Path("data") / "nps"
    
    @property
    def raw_history_dir(self) -> Path:
        return self.base_dir / "history" / "in"
    
    @property
    def processed_history_dir(self) -> Path:
        return self.base_dir / "history" / "out"

@dataclass
class NPSURLConfig:
    """NPS ê´€ë ¨ URL ì„¤ì •"""
    data_gov_base: str = "https://www.data.go.kr"
    
    @property
    def main_url(self) -> str:
        return f"{self.data_gov_base}/data/15083277/fileData.do"
    
    @property
    def hist_url(self) -> str:
        return f"{self.data_gov_base}/tcs/dss/selectHistAndCsvData.do"
    
    @property
    def detail_url(self) -> str:
        return f"{self.data_gov_base}/tcs/dss/selectDpkDetailInfo.do"
    
    @property
    def download_json_url(self) -> str:
        return f"{self.data_gov_base}/tcs/dss/selectFileDataDownload.do"
    
    @property
    def file_download_url(self) -> str:
        return f"{self.data_gov_base}/cmm/cmm/fileDownload.do"

@dataclass
class NPSSpecialCases:
    """NPS íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì„¤ì •"""
    special_case_label: str = "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨_êµ­ë¯¼ì—°ê¸ˆ ê°€ì… ì‚¬ì—…ì¥ ë‚´ì—­_20200624"
    special_version_202007: str = "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨_êµ­ë¯¼ì—°ê¸ˆ ê°€ì… ì‚¬ì—…ì¥ ë‚´ì—­_20200717"
    special_version_202006: str = "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨_êµ­ë¯¼ì—°ê¸ˆ ê°€ì… ì‚¬ì—…ì¥ ë‚´ì—­_20200619"
    
    @property
    def special_cases_config(self) -> dict:
        return {
            self.special_case_label: {
                "reason": "2020ë…„ 6ì›” ë°ì´í„° ì¤‘ë³µ ì—…ë¡œë“œ ë¬¸ì œ",
                "versions": [
                    {
                        "pattern": "2020-07",
                        "filename": self.special_version_202007,
                        "description": "ìˆ˜ì •ë³¸ (ìµœì‹ )"
                    },
                    {
                        "pattern": "2020-06",
                        "filename": self.special_version_202006, 
                        "description": "ì´ˆê¸°ë³¸"
                    }
                ]
            }
        }
    
    @property
    def metadata_file_patterns(self) -> list[str]:
        return [
            "ì»¬ëŸ¼ì •ë³´", "í•­ëª©ì •ë³´", "íŒŒì¼ì •ë³´", "ë°ì´í„°ì •ë³´", "ìŠ¤í‚¤ë§ˆ",
            "schema", "column", "metadata", "ì„¤ëª…ì„œ", "ê°€ì´ë“œ", "manual"
        ]

@dataclass
class NPSSchema:
    """NPS ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜"""
    original_columns: list[str] = field(default_factory=lambda: [
        "ì‚¬ì—…ì¥ëª…", "ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸", "ìš°í¸ë²ˆí˜¸", "ì‚¬ì—…ì¥ì§€ë²ˆìƒì„¸ì£¼ì†Œ", "ì‚¬ì—…ì¥ë„ë¡œëª…ìƒì„¸ì£¼ì†Œ", 
        "ê³ ê°ë²•ì •ë™ì£¼ì†Œì½”ë“œ", "ê³ ê°í–‰ì •ë™ì£¼ì†Œì½”ë“œ", "ì‚¬ì—…ì¥í˜•íƒœêµ¬ë¶„ì½”ë“œ", "ì‚¬ì—…ì¥ì—…ì¢…ì½”ë“œ", 
        "ì‚¬ì—…ì¥ì—…ì¢…ì½”ë“œëª…", "ì ìš©ì¼ì", "ì¬ë“±ë¡ì¼ì", "íƒˆí‡´ì¼ì", "ê°€ì…ììˆ˜", "ë‹¹ì›”ê³ ì§€ê¸ˆì•¡", 
        "ì‹ ê·œì·¨ë“ììˆ˜", "ìƒì‹¤ê°€ì…ììˆ˜"
    ])
    
    column_mapping: dict[str, str] = field(default_factory=lambda: {
        'ìë£Œìƒì„±ë…„ì›”': 'date_extracted',
        'ì‚¬ì—…ì¥ëª…': 'company_name',
        'ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸': 'business_reg_num',
        'ìš°í¸ë²ˆí˜¸': 'postal_code',
        'ì‚¬ì—…ì¥ì§€ë²ˆìƒì„¸ì£¼ì†Œ': 'address_jibun',
        'ì‚¬ì—…ì¥ë„ë¡œëª…ìƒì„¸ì£¼ì†Œ': 'address_road',
        'ê³ ê°ë²•ì •ë™ì£¼ì†Œì½”ë“œ': 'legal_dong_code',
        'ê³ ê°í–‰ì •ë™ì£¼ì†Œì½”ë“œ': 'admin_dong_code',
        'ì‚¬ì—…ì¥í˜•íƒœêµ¬ë¶„ì½”ë“œ': 'company_type_code',
        'ì‚¬ì—…ì¥ì—…ì¢…ì½”ë“œ': 'industry_code',
        'ì‚¬ì—…ì¥ì—…ì¢…ì½”ë“œëª…': 'industry_name',
        'ì ìš©ì¼ì': 'application_date',
        'ì¬ë“±ë¡ì¼ì': 'reregistration_date',
        'íƒˆí‡´ì¼ì': 'withdrawal_date',
        'ê°€ì…ììˆ˜': 'subscriber_count',
        'ë‹¹ì›”ê³ ì§€ê¸ˆì•¡': 'monthly_fee',
        'ì‹ ê·œì·¨ë“ììˆ˜': 'new_members',
        'ìƒì‹¤ê°€ì…ììˆ˜': 'lost_members'
    })
    
    numeric_columns: list[str] = field(default_factory=lambda: [
        'ê°€ì…ììˆ˜', 'ë‹¹ì›”ê³ ì§€ê¸ˆì•¡', 'ì‹ ê·œì·¨ë“ììˆ˜', 'ìƒì‹¤ê°€ì…ììˆ˜'
    ])
    
    string_columns: list[str] = field(default_factory=lambda: [
        'ì‚¬ì—…ì¥ëª…', 'ì‚¬ì—…ì¥ì§€ë²ˆìƒì„¸ì£¼ì†Œ', 'ì‚¬ì—…ì¥ë„ë¡œëª…ìƒì„¸ì£¼ì†Œ', 'ì‚¬ì—…ì¥ì—…ì¢…ì½”ë“œëª…'
    ])
    
    @property
    def processing_dtypes(self) -> dict[str, Any]:
        return {
            "ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸": "Int64",
            "ê°€ì…ììˆ˜": "Int64",
            "ë‹¹ì›”ê³ ì§€ê¸ˆì•¡": "Int64",
            "ì‹ ê·œì·¨ë“ììˆ˜": "Int64",
            "ìƒì‹¤ê°€ì…ììˆ˜": "Int64",
            "ì ìš©ì¼ì": str,
            "ì¬ë“±ë¡ì¼ì": str,
            "íƒˆí‡´ì¼ì": str,
            "ìë£Œìƒì„±ë…„ì›”": str,
        }

@dataclass
class NPSDatabase:
    """NPS ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ì„¤ì •"""
    metadata_table: str = "nps_file_metadata"
    processed_table: str = "nps_processed_data"
    
    @property
    def postgres_column_order(self) -> list[str]:
        return [
            "date", "company_id", "company_name", "business_reg_num", "postal_code", 
            "address_jibun", "address_road", "legal_dong_code", "admin_dong_code", 
            "company_type_code", "industry_code", "industry_name", "application_date", 
            "reregistration_date", "withdrawal_date", "subscriber_count", "monthly_fee", 
            "new_members", "lost_members", "avg_fee"
        ]

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ë“¤
CONFIG = NPSConfig()
PATHS = NPSFilePaths()
URLS = NPSURLConfig()
SPECIAL_CASES = NPSSpecialCases()
SCHEMA = NPSSchema()
DATABASE = NPSDatabase()

# ì •ê·œì‹ íŒ¨í„´ë“¤
YYYYMMDD_PATTERN = re.compile(r"(\d{4})(\d{2})(\d{2})")
MMDDYYYY_PATTERN = re.compile(r"(\d{1,2})[/_-](\d{1,2})[/_-](\d{4})")  
YYYYMM_PATTERN = re.compile(r"(\d{4})\D*(\d{1,2})\D")  
KOREAN_DATE_PATTERN = re.compile(r"(\d{4})ë…„\s*(\d{1,2})ì›”")
UNDERLINE_DATE_PATTERN = re.compile(r"(\d{1,2})_(\d{1,2})_(\d{4})")

# =============================================================================
# 2ë‹¨ê³„: ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤
# =============================================================================

def _is_metadata_file(filename: str) -> bool:
    """ë©”íƒ€ë°ì´í„°/ì„¤ëª…ì„œ íŒŒì¼ì¸ì§€ íŒë‹¨"""
    filename_lower = filename.lower()
    return any(pattern in filename_lower for pattern in SPECIAL_CASES.metadata_file_patterns)

def _sanitize_filename(filename: str) -> str:
    """íŒŒì¼ëª…ì—ì„œ ì•ˆì „í•˜ì§€ ì•Šì€ ë¬¸ìë“¤ì„ ì œê±°í•˜ê±°ë‚˜ ëŒ€ì²´"""
    filename = filename.replace("/", "_")
    filename = filename.replace("\\", "_")
    filename = re.sub(r'[<>:"|?*]', "", filename)
    filename = re.sub(r'\s+', " ", filename)
    filename = filename.strip()
    return filename

def _check_disk_space(path: Path, required_gb: float = None) -> bool:
    """ë””ìŠ¤í¬ ê³µê°„ ì²´í¬"""
    if required_gb is None:
        required_gb = CONFIG.required_disk_space_gb
        
    try:
        total, used, free = shutil.disk_usage(path)
        free_gb = free / (1024**3)  # GB ë³€í™˜
        
        logging.getLogger(__name__).info(
            f"ë””ìŠ¤í¬ ê³µê°„ ì²´í¬: {free_gb:.2f}GB ì—¬ìœ  ê³µê°„ (í•„ìš”: {required_gb}GB)"
        )
        
        return free_gb >= required_gb
    except Exception as e:
        logging.getLogger(__name__).warning(f"ë””ìŠ¤í¬ ê³µê°„ ì²´í¬ ì‹¤íŒ¨: {e}")
        return True

def _get_expected_file_size(
    session: requests.Session,
    file_info: dict[str, str],
    timeout: int = None
) -> int | None:
    """íŒŒì¼ì˜ ì˜ˆìƒ í¬ê¸°ë¥¼ HEAD ìš”ì²­ìœ¼ë¡œ í™•ì¸"""
    if timeout is None:
        timeout = CONFIG.request_timeout
        
    try:
        # Step 1: ìƒì„¸ í˜ì´ì§€ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        detail_response = session.get(URLS.detail_url, params={
            "publicDataDetailPk": file_info['detail_pk'],
            "publicDataHistSn": file_info['hist_sn']
        }, timeout=timeout)
        detail_response.raise_for_status()
        
        detail_soup = BeautifulSoup(detail_response.text, "html.parser")
        download_btn = detail_soup.find("a", onclick=re.compile(r"fn_fileDataDown"))
        
        if not download_btn:
            return None
        
        onclick_params = re.findall(r"'([^']+)'", download_btn["onclick"])
        if len(onclick_params) < 5:
            return None
        
        pub_pk, detail_pk_2, atch_id, sn, ext = onclick_params[:5]
        
        # Step 2: JSON API í˜¸ì¶œ
        json_response = session.get(URLS.download_json_url, params={
            "publicDataPk": pub_pk,
            "publicDataDetailPk": detail_pk_2,
            "fileExtsn": ext
        }, timeout=timeout)
        json_response.raise_for_status()
        
        json_data = json_response.json()
        if "atchFileId" not in json_data:
            return None
        
        # Step 3: HEAD ìš”ì²­ìœ¼ë¡œ íŒŒì¼ í¬ê¸° í™•ì¸
        head_response = session.head(URLS.file_download_url, params={
            "atchFileId": json_data["atchFileId"]
        }, timeout=timeout)
        head_response.raise_for_status()
        
        # Content-Length í—¤ë”ì—ì„œ íŒŒì¼ í¬ê¸° ì¶”ì¶œ
        content_length = head_response.headers.get('Content-Length')
        if content_length:
            return int(content_length)
        
        return None
        
    except Exception as e:
        logging.getLogger(__name__).debug(f"íŒŒì¼ í¬ê¸° í™•ì¸ ì‹¤íŒ¨: {e}")
        return None

# =============================================================================
# 3ë‹¨ê³„: ë‹¤ìš´ë¡œë“œ í›„ë³´ ì¶”ì¶œ í•¨ìˆ˜ (ìˆ˜ì •ë¨ - nps_download.pyì—ì„œ ê°€ì ¸ì˜´)
# =============================================================================

def _extract_download_candidates(
    session: requests.Session, 
    main_pk: str, 
    detail_pk: str
) -> list[dict[str, str]]:
    """NPS íˆìŠ¤í† ë¦¬ í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ë³´ë“¤ ì¶”ì¶œ (nps_download.py ì‘ë™ ë²„ì „ ì‚¬ìš©)"""
    logger = logging.getLogger(__name__)
    
    try:
        # íˆìŠ¤í† ë¦¬ URL ì‚¬ìš© (nps_download.pyì™€ ë™ì¼)
        response = session.get(URLS.hist_url, params={
            "publicDataPk": main_pk,
            "publicDataDetailPk": detail_pk
        }, timeout=CONFIG.request_timeout)
        logger.info(f"íˆìŠ¤í† ë¦¬ í˜ì´ì§€ ìš”ì²­: {URLS.hist_url}")
        logger.info(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
        response.raise_for_status()
        
        # HTML íŒŒì‹± ë° íˆìŠ¤í† ë¦¬ ë§í¬ ì°¾ê¸° (nps_download.pyì™€ ë™ì¼í•œ ë°©ì‹)
        soup = BeautifulSoup(response.text, "html.parser")
        history_links = soup.select("div.data-history a[onclick*='fn_fileDataDetail']")
        
        logger.info(f"ğŸ“‹ ì „ì²´ íˆìŠ¤í† ë¦¬ ë§í¬ ë°œê²¬: {len(history_links)}ê°œ")
        
        if not history_links:
            logger.error("íˆìŠ¤í† ë¦¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ë””ë²„ê¹…ì„ ìœ„í•´ í˜ì´ì§€ ë‚´ìš©ì˜ ì¼ë¶€ë¥¼ ë¡œê¹…
            logger.error(f"í˜ì´ì§€ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 1000ì): {response.text[:1000]}")
            
            # ì „ì²´ í˜ì´ì§€ì—ì„œ 'data-history' í…ìŠ¤íŠ¸ ê²€ìƒ‰
            if 'data-history' in response.text:
                logger.error("í˜ì´ì§€ì— 'data-history' í…ìŠ¤íŠ¸ê°€ ì¡´ì¬í•©ë‹ˆë‹¤")
            else:
                logger.error("í˜ì´ì§€ì— 'data-history' í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # div.data-history ìš”ì†Œ ì°¾ê¸° ì‹œë„
            data_history_div = soup.find("div", class_="data-history")
            if data_history_div:
                logger.error(f"data-history div ë°œê²¬, í•˜ìœ„ ë§í¬ ìˆ˜: {len(data_history_div.find_all('a'))}")
                logger.error(f"data-history div ë‚´ìš©: {str(data_history_div)[:500]}")
            else:
                logger.error("data-history divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
                # ë‹¤ë¥¸ history ê´€ë ¨ í´ë˜ìŠ¤ ì°¾ê¸°
                history_divs = soup.find_all("div", class_=lambda x: x and "history" in x.lower())
                if history_divs:
                    logger.error(f"ë‹¤ë¥¸ history ê´€ë ¨ divë“¤: {[div.get('class') for div in history_divs]}")
                else:
                    logger.error("history ê´€ë ¨ divê°€ ì—†ìŠµë‹ˆë‹¤")
            
            return []
        
        candidates = []
        processed_links = set()
        metadata_files_skipped = []
        unprocessed_links = []
        
        # ğŸ”¥ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì„¤ì • (ë™ì  ì²˜ë¦¬ ê°€ëŠ¥)
        special_cases = {
            SPECIAL_CASES.special_case_label: {
                "versions": [
                    {
                        "pattern": "2020-07",
                        "filename": SPECIAL_CASES.special_version_202007,
                        "processed": False,
                        "description": "2020ë…„ 7ì›” ìˆ˜ì •ë³¸"
                    },
                    {
                        "pattern": "2020-06", 
                        "filename": SPECIAL_CASES.special_version_202006,
                        "processed": False,
                        "description": "2020ë…„ 6ì›” ì´ˆê¸°ë³¸"
                    }
                ]
            }
        }
        
        # ğŸ” ëª¨ë“  ë§í¬ ìƒì„¸ ë¶„ì„
        for i, link in enumerate(history_links, 1):
            text = link.text.strip()
            onclick_attr = link.get("onclick", "")
            parent_text = link.parent.get_text(strip=True) if link.parent else ""
            
            logger.debug(f"ğŸ“ ë§í¬ {i}: '{text}' (ë¶€ëª¨: '{parent_text}') [onclick: {onclick_attr[:50]}...]")
            
            # ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ê±´ë„ˆë›¸ ë¼ë²¨ ì²´í¬
            if not text or text == CONFIG.skip_label:
                logger.debug(f"â­ï¸  ë¹ˆ í…ìŠ¤íŠ¸ ë˜ëŠ” ê±´ë„ˆë›¸ ë¼ë²¨: '{text}'")
                continue
                
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ í•„í„°ë§
            if _is_metadata_file(text):
                metadata_files_skipped.append({
                    "text": text,
                    "parent_text": parent_text,
                    "link_index": i
                })
                logger.info(f"ğŸ“„ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê±´ë„ˆëœ€: '{text}'")
                continue
                
            # onclick íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            onclick_matches = re.findall(r"'([^']+)'", onclick_attr)
            if len(onclick_matches) < 2:
                logger.warning(f"âš ï¸  onclick íŒŒë¼ë¯¸í„° ë¶€ì¡± ({len(onclick_matches)}ê°œ): '{text}' - {onclick_attr}")
                unprocessed_links.append({
                    "text": text,
                    "reason": f"onclick íŒŒë¼ë¯¸í„° ë¶€ì¡± ({len(onclick_matches)}ê°œ)",
                    "onclick": onclick_attr
                })
                continue
                
            detail_pk_candidate = onclick_matches[0]
            hist_sn_candidate = onclick_matches[1]
            
            # ğŸ¯ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ (ê°•í™”ëœ ë¡œì§)
            special_case_processed = False
            
            if text in special_cases:
                logger.info(f"ğŸ” íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ í›„ë³´ ë°œê²¬: '{text}'")
                logger.info(f"   ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸: '{parent_text}'")
                
                special_case = special_cases[text]
                
                # ê° ë²„ì „ë³„ë¡œ íŒ¨í„´ ë§¤ì¹­ ì‹œë„
                for version in special_case["versions"]:
                    if version["processed"]:
                        continue
                    
                    pattern = version["pattern"]
                    filename = version["filename"]
                    
                    # ë‹¤ì–‘í•œ íŒ¨í„´ ë§¤ì¹­ ì‹œë„
                    pattern_found = False
                    
                    # 1. ë¶€ëª¨ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ì°¾ê¸°
                    if pattern in parent_text:
                        pattern_found = True
                        logger.info(f"   âœ… ë¶€ëª¨ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ '{pattern}' ë°œê²¬")
                    
                    # 2. ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ì°¾ê¸°
                    elif pattern in text:
                        pattern_found = True
                        logger.info(f"   âœ… ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ '{pattern}' ë°œê²¬")
                    
                    # 3. ë‚ ì§œ í˜•ì‹ ë³€í™˜ í›„ ì°¾ê¸° (2020-07 â†’ 202007, 20200701 ë“±)
                    elif any(date_variant in parent_text or date_variant in text 
                            for date_variant in [
                                pattern.replace("-", ""),  # 2020-07 â†’ 202007
                                pattern.replace("-", "") + "01",  # 2020-07 â†’ 20200701
                                pattern.replace("-", "/"),  # 2020-07 â†’ 2020/07
                            ]):
                        pattern_found = True
                        logger.info(f"   âœ… ë‚ ì§œ ë³€í˜•ì—ì„œ íŒ¨í„´ '{pattern}' ë°œê²¬")
                    
                    if pattern_found:
                        candidates.append({
                            "filename": filename,
                            "detail_pk": detail_pk_candidate,
                            "hist_sn": hist_sn_candidate,
                            "is_special_case": True
                        })
                        
                        version["processed"] = True
                        special_case_processed = True
                        
                        logger.info(f"   ğŸ¯ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì¶”ê°€ ì™„ë£Œ: {filename} (íŒ¨í„´: {pattern})")
                        break
                
                # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ë¼ë²¨ì´ì§€ë§Œ íŒ¨í„´ì´ ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš°
                if not special_case_processed:
                    logger.warning(f"âš ï¸  íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ë¼ë²¨ '{text}'ì´ì§€ë§Œ íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨")
                    logger.warning(f"   ğŸ“ í™•ì¸ëœ í…ìŠ¤íŠ¸: '{parent_text}'")
                    unprocessed_links.append({
                        "text": text,
                        "reason": "íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨",
                        "parent_text": parent_text
                    })
            
            # ì¼ë°˜ íŒŒì¼ ì²˜ë¦¬ (íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš°)
            if not special_case_processed:
                if text in processed_links:
                    logger.debug(f"ğŸ”„ ì¤‘ë³µ íŒŒì¼ëª… ê±´ë„ˆëœ€: '{text}'")
                    continue
                    
                candidates.append({
                    "filename": text,
                    "detail_pk": detail_pk_candidate,
                    "hist_sn": hist_sn_candidate,
                    "is_special_case": False
                })
                processed_links.add(text)
                logger.debug(f"âœ… ì¼ë°˜ íŒŒì¼ ì¶”ê°€: '{text}'")
        
        # ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…
        logger.info(f"ì´ {len(candidates)}ê°œ ë‹¤ìš´ë¡œë“œ í›„ë³´ ë°œê²¬")
        logger.info(f"ë©”íƒ€ë°ì´í„° ì œì™¸: {len(metadata_files_skipped)}ê°œ")
        logger.info(f"ì²˜ë¦¬ ì‹¤íŒ¨: {len(unprocessed_links)}ê°œ")
        
        return candidates
        
    except Exception as e:
        logger.error(f"ë‹¤ìš´ë¡œë“œ í›„ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        logger.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        logger.error(f"ì—ëŸ¬ ìƒì„¸: {str(e)}")
        import traceback
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        return []

# =============================================================================
# 4ë‹¨ê³„: ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
# =============================================================================

def _download_single_nps_file(
    session: requests.Session,
    file_info: dict[str, str],
    thread_id: int,
    stop_event: threading.Event = None
) -> dict[str, Any]:
    """ë‹¨ì¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    if stop_event and stop_event.is_set():
        return {"success": False, "error": "ì¤‘ë‹¨ë¨"}
    
    filename = file_info["filename"]
    detail_pk = file_info["detail_pk"]
    hist_sn = file_info["hist_sn"]
    is_special_case = file_info.get("is_special_case", False)
    
    try:
        # Step 1: ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        detail_response = session.get(URLS.detail_url, params={
            "publicDataDetailPk": detail_pk,
            "publicDataHistSn": hist_sn
        }, timeout=CONFIG.request_timeout)
        detail_response.raise_for_status()
        
        detail_soup = BeautifulSoup(detail_response.text, "html.parser")
        download_btn = detail_soup.find("a", onclick=re.compile(r"fn_fileDataDown"))
        
        if not download_btn:
            raise RuntimeError("ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        onclick_params = re.findall(r"'([^']+)'", download_btn["onclick"])
        if len(onclick_params) < 5:
            raise RuntimeError("ë‹¤ìš´ë¡œë“œ íŒŒë¼ë¯¸í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
        
        pub_pk, detail_pk_2, atch_id, sn, ext = onclick_params[:5]
        
        # Step 2: JSON APIë¡œ ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URL íšë“
        json_response = session.get(URLS.download_json_url, params={
            "publicDataPk": pub_pk,
            "publicDataDetailPk": detail_pk_2,
            "fileExtsn": ext
        }, timeout=CONFIG.request_timeout)
        json_response.raise_for_status()
        
        json_data = json_response.json()
        if "atchFileId" not in json_data:
            raise RuntimeError("ì²¨ë¶€íŒŒì¼ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # Step 3: ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        download_response = session.get(URLS.file_download_url, params={
            "atchFileId": json_data["atchFileId"]
        }, timeout=CONFIG.request_timeout, stream=True)
        download_response.raise_for_status()
        
        # íŒŒì¼ ì €ì¥
        sanitized_filename = _sanitize_filename(filename)
        if not sanitized_filename.endswith('.csv'):
            sanitized_filename += '.csv'
        
        file_path = PATHS.raw_history_dir / sanitized_filename
        
        # íŒŒì¼ ì“°ê¸°
        with open(file_path, 'wb') as f:
            for chunk in download_response.iter_content(chunk_size=8192):
                if stop_event and stop_event.is_set():
                    f.close()
                    file_path.unlink(missing_ok=True)  # ë¶€ë¶„ íŒŒì¼ ì‚­ì œ
                    return {"success": False, "error": "ì¤‘ë‹¨ë¨"}
                f.write(chunk)
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = file_path.stat().st_size
        if file_size < CONFIG.min_file_size_kb * 1024:
            file_path.unlink(missing_ok=True)
            raise RuntimeError(f"ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {file_size}B")
        
        logging.getLogger(__name__).info(
            f"[Thread-{thread_id}] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename} â†’ {sanitized_filename} ({file_size:,}B)"
        )
        
        return {
            "success": True,
            "filename": filename,
            "sanitized_filename": sanitized_filename,
            "path": str(file_path),
            "file_size": file_size,
            "thread_id": thread_id,
            "is_special_case": is_special_case,
            "integrity_check": "size_verified"
        }
        
    except Exception as e:
        error_msg = f"[Thread-{thread_id}] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}"
        
        if is_special_case:
            logging.getLogger(__name__).warning(f"{error_msg} (íŠ¹ìˆ˜ ì¼€ì´ìŠ¤)")
        else:
            logging.getLogger(__name__).error(error_msg)
        
        # Fail-Fast: ëª¨ë“  ì˜ˆì™¸ë¥¼ ì¦‰ì‹œ ì¬ë°œìƒ
        raise RuntimeError(error_msg) from e

@contextmanager
def _graceful_executor(max_workers: int):
    """ThreadPoolExecutorë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    executor = ThreadPoolExecutor(max_workers=max_workers)
    try:
        yield executor
    except KeyboardInterrupt:
        logging.getLogger(__name__).warning("í‚¤ë³´ë“œ ì¤‘ë‹¨ ê°ì§€ - ëª¨ë“  ì‘ì—… ì¢…ë£Œ ì¤‘...")
        executor.shutdown(wait=False, cancel_futures=True)
        raise
    except Exception:
        executor.shutdown(wait=False, cancel_futures=True)
        raise
    finally:
        executor.shutdown(wait=True)

def _get_existing_files_from_db(conn) -> set[str]:
    """DuckDBì—ì„œ ê¸°ì¡´ ì²˜ë¦¬ëœ íŒŒì¼ë“¤ì˜ ëª©ë¡ì„ ì¡°íšŒ"""
    try:
        tables_query = f"""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_name = '{DATABASE.metadata_table}'
        """
        result = conn.execute(tables_query).fetchone()
        
        if not result:
            return set()
        
        existing_files_query = f"""
        SELECT DISTINCT original_filename, sanitized_filename
        FROM {DATABASE.metadata_table}
        WHERE success = true
        """
        
        result = conn.execute(existing_files_query).fetchall()
        
        existing_files = set()
        for row in result:
            existing_files.add(row[0])  # original_filename
            existing_files.add(row[1])  # sanitized_filename
        
        return existing_files
        
    except Exception as e:
        logging.getLogger(__name__).warning(f"ê¸°ì¡´ íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return set()

def _get_existing_files_from_filesystem() -> set[str]:
    """íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ê¸°ì¡´ íŒŒì¼ë“¤ì˜ ëª©ë¡ì„ ì¡°íšŒ"""
    logger = logging.getLogger(__name__)
    existing_files = set()
    
    try:
        # raw_history_dirì— ìˆëŠ” ëª¨ë“  CSV íŒŒì¼ë“¤ í™•ì¸
        if PATHS.raw_history_dir.exists():
            for file_path in PATHS.raw_history_dir.glob("*.csv"):
                if file_path.is_file() and file_path.stat().st_size > CONFIG.min_file_size_kb * 1024:
                    # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°í•œ ë²„ì „ê³¼ ì›ë³¸ íŒŒì¼ëª… ëª¨ë‘ ì¶”ê°€
                    existing_files.add(file_path.name)  # ì‹¤ì œ íŒŒì¼ëª… (sanitized)
                    existing_files.add(file_path.stem)  # í™•ì¥ì ì œê±°í•œ íŒŒì¼ëª…
                    
                    # ë‚ ì§œ ì ‘ë‘ì‚¬ê°€ ìˆëŠ” ê²½ìš° ì œê±°í•œ ë²„ì „ë„ ì¶”ê°€
                    stem = file_path.stem
                    if '_' in stem:
                        # ì˜ˆ: "20200619_êµ­ë¯¼ì—°ê¸ˆê³µë‹¨_êµ­ë¯¼ì—°ê¸ˆ ê°€ì… ì‚¬ì—…ì¥ ë‚´ì—­_20200717" 
                        # â†’ "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨_êµ­ë¯¼ì—°ê¸ˆ ê°€ì… ì‚¬ì—…ì¥ ë‚´ì—­_20200717"
                        parts = stem.split('_', 1)
                        if len(parts) > 1 and parts[0].isdigit():
                            existing_files.add(parts[1])
        
        # processed_history_dirì— ìˆëŠ” íŒŒì¼ë“¤ë„ í™•ì¸
        if PATHS.processed_history_dir.exists():
            for file_path in PATHS.processed_history_dir.glob("*.csv"):
                if file_path.is_file():
                    existing_files.add(file_path.name)
                    existing_files.add(file_path.stem)
                    
                    # ë‚ ì§œ ì ‘ë‘ì‚¬ ì œê±° ë²„ì „
                    stem = file_path.stem
                    if '_' in stem:
                        parts = stem.split('_', 1)
                        if len(parts) > 1 and parts[0].isdigit():
                            existing_files.add(parts[1])
        
        logger.info(f"íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ {len(existing_files)}ê°œ ê¸°ì¡´ íŒŒì¼ëª… íŒ¨í„´ ë°œê²¬")
        return existing_files
        
    except Exception as e:
        logger.warning(f"íŒŒì¼ ì‹œìŠ¤í…œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return set()

def _get_combined_existing_files(conn) -> tuple[set[str], list[dict[str, Any]]]:
    """íŒŒì¼ì‹œìŠ¤í…œê³¼ DuckDBì—ì„œ ê¸°ì¡´ íŒŒì¼ë“¤ì˜ ëª©ë¡ì„ ì¡°íšŒ (ë¬´ê²°ì„± ê²€ì¦ í¬í•¨)"""
    logger = logging.getLogger(__name__)
    
    # 1. íŒŒì¼ì‹œìŠ¤í…œì—ì„œ ê¸°ì¡´ íŒŒì¼ë“¤ ì¡°íšŒ ë° ë¬´ê²°ì„± ê²€ì¦
    filesystem_files = set()
    corrupted_files = []
    
    if PATHS.raw_history_dir.exists():
        logger.info(f"íŒŒì¼ì‹œìŠ¤í…œ ê²€ì‚¬ ì¤‘: {PATHS.raw_history_dir}")
        csv_files = list(PATHS.raw_history_dir.glob("*.csv"))
        logger.info(f"ë°œê²¬ëœ CSV íŒŒì¼ ìˆ˜: {len(csv_files)}ê°œ")
        
        for file_path in csv_files:
            validation_result = _validate_file_integrity(file_path)
            
            if validation_result["valid"]:
                # ë‹¤ì–‘í•œ íŒŒì¼ëª… íŒ¨í„´ ì¶”ê°€ (ì¤‘ë³µ ì²´í¬ ê°œì„ )
                filename = file_path.name
                stem = file_path.stem
                
                filesystem_files.add(filename)  # ì „ì²´ íŒŒì¼ëª…
                filesystem_files.add(stem)      # í™•ì¥ì ì œê±°
                
                # ë‚ ì§œ ì ‘ë‘ì‚¬ ì œê±° íŒ¨í„´ë“¤
                if '_' in stem:
                    parts = stem.split('_', 1)
                    if len(parts) > 1 and parts[0].isdigit():
                        filesystem_files.add(parts[1])  # ë‚ ì§œ ì ‘ë‘ì‚¬ ì œê±°
                        filesystem_files.add(parts[1] + '.csv')  # ë‚ ì§œ ì ‘ë‘ì‚¬ ì œê±° + í™•ì¥ì
                
                # íŠ¹ìˆ˜ ë¬¸ì ì œê±°/ì¹˜í™˜ëœ ë²„ì „ë„ ì¶”ê°€
                sanitized = _sanitize_filename(stem)
                filesystem_files.add(sanitized)
                filesystem_files.add(sanitized + '.csv')
                
                logger.debug(f"ìœ íš¨í•œ íŒŒì¼ ë“±ë¡: {filename}")
            else:
                corrupted_files.append({
                    "filename": file_path.name,
                    "path": str(file_path),
                    "validation_result": validation_result
                })
                logger.warning(
                    f"ì†ìƒëœ íŒŒì¼ ë°œê²¬: {file_path.name} - {validation_result.get('reason', 'unknown')}"
                )
    
    # 2. DuckDBì—ì„œ ê¸°ì¡´ íŒŒì¼ë“¤ ì¡°íšŒ
    db_files = set()
    try:
        tables_query = f"""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_name = '{DATABASE.metadata_table}'
        """
        result = conn.execute(tables_query).fetchone()
        
        if result:
            existing_files_query = f"""
            SELECT DISTINCT original_filename, sanitized_filename
            FROM {DATABASE.metadata_table}
            WHERE success = true
            """
            
            result = conn.execute(existing_files_query).fetchall()
            
            for row in result:
                original_filename = row[0]
                sanitized_filename = row[1]
                
                # ë‹¤ì–‘í•œ íŒ¨í„´ ì¶”ê°€
                db_files.add(original_filename)
                db_files.add(sanitized_filename)
                
                # í™•ì¥ì ê´€ë ¨ íŒ¨í„´ë“¤
                if original_filename.endswith('.csv'):
                    db_files.add(original_filename[:-4])  # í™•ì¥ì ì œê±°
                if not sanitized_filename.endswith('.csv'):
                    db_files.add(sanitized_filename + '.csv')  # í™•ì¥ì ì¶”ê°€
            
            logger.info(f"DBì—ì„œ {len(result)}ê°œ ì„±ê³µ ë ˆì½”ë“œ ë°œê²¬")
    except Exception as e:
        logger.warning(f"DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    # 3. í†µí•© ê²°ê³¼ ë°˜í™˜ (ì†ìƒëœ íŒŒì¼ì€ ì œì™¸)
    valid_files = filesystem_files.union(db_files)
    
    logger.info(
        f"íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ: íŒŒì¼ì‹œìŠ¤í…œ {len(filesystem_files)}ê°œ íŒ¨í„´, "
        f"DB {len(db_files)}ê°œ íŒ¨í„´, ì†ìƒëœ íŒŒì¼ {len(corrupted_files)}ê°œ"
    )
    logger.info(f"í†µí•©ëœ ìœ íš¨ íŒŒì¼ íŒ¨í„´: {len(valid_files)}ê°œ")
    
    return valid_files, corrupted_files

def _update_metadata_batch(conn, successful_downloads: list[dict[str, Any]]) -> None:
    """ì„±ê³µí•œ ë‹¤ìš´ë¡œë“œë“¤ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ DuckDBì— ë°°ì¹˜ ì €ì¥"""
    if not successful_downloads:
        return
    
    try:
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {DATABASE.metadata_table} (
            id BIGINT,
            original_filename VARCHAR,
            sanitized_filename VARCHAR,
            file_path VARCHAR,
            success BOOLEAN,
            download_timestamp TIMESTAMP,
            file_size_bytes BIGINT,
            thread_id INTEGER,
            is_special_case BOOLEAN,
            special_version VARCHAR,
            integrity_check VARCHAR,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            PRIMARY KEY (id)
        )
        """
        conn.execute(create_table_sql)
        
        max_id_result = conn.execute(f"SELECT COALESCE(MAX(id), 0) FROM {DATABASE.metadata_table}").fetchone()
        next_id = max_id_result[0] + 1 if max_id_result else 1
        
        insert_data = []
        current_timestamp = datetime.now(timezone.utc)
        
        for i, download in enumerate(successful_downloads):
            insert_data.append((
                next_id + i,
                download["filename"],
                download["sanitized_filename"],
                download["path"],
                True,
                current_timestamp,
                download["file_size"],
                download["thread_id"],
                download.get("is_special_case", False),
                "",  # special_versionì€ ì¶”í›„ í™•ì¥ ê°€ëŠ¥
                download.get("integrity_check", "")
            ))
        
        insert_sql = f"""
        INSERT INTO {DATABASE.metadata_table} (
            id, original_filename, sanitized_filename, file_path, success,
            download_timestamp, file_size_bytes, thread_id, is_special_case,
            special_version, integrity_check, created_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        
        conn.executemany(insert_sql, insert_data)
        
        logging.getLogger(__name__).info(
            f"ë©”íƒ€ë°ì´í„° ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(insert_data)}ê°œ ë ˆì½”ë“œ"
        )
        
    except Exception as e:
        logging.getLogger(__name__).error(f"ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def _validate_file_integrity(file_path: Path) -> dict[str, Any]:
    """íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦"""
    try:
        if not file_path.exists():
            return {"valid": False, "reason": "file_not_found"}
        
        file_size = file_path.stat().st_size
        
        # 1. ìµœì†Œ í¬ê¸° ê²€ì¦
        if file_size < CONFIG.min_file_size_kb * 1024:
            return {
                "valid": False, 
                "reason": "file_too_small", 
                "size_bytes": file_size,
                "min_size_bytes": CONFIG.min_file_size_kb * 1024
            }
        
        # 2. CSV í—¤ë” ê²€ì¦ (ì²« ëª‡ ì¤„ë§Œ ì½ì–´ì„œ í™•ì¸)
        try:
            with open(file_path, 'r', encoding='utf-8-sig', errors='replace') as f:
                first_line = f.readline().strip()
                if not first_line:
                    return {"valid": False, "reason": "empty_file"}
                
                # CSV í—¤ë”ê°€ ìˆëŠ”ì§€ ê°„ë‹¨íˆ í™•ì¸
                if ',' not in first_line and '\t' not in first_line:
                    return {"valid": False, "reason": "invalid_csv_format"}
        
        except Exception as read_error:
            return {"valid": False, "reason": "read_error", "error": str(read_error)}
        
        # 3. íŒŒì¼ í™•ì¥ì ê²€ì¦
        if not file_path.name.lower().endswith('.csv'):
            return {"valid": False, "reason": "invalid_extension"}
        
        return {
            "valid": True, 
            "size_bytes": file_size,
            "size_mb": round(file_size / (1024 * 1024), 2)
        }
        
    except Exception as e:
        return {"valid": False, "reason": "validation_error", "error": str(e)}

# =============================================================================
# NPS íŒŒì¼ ë‹¤ìš´ë¡œë“œ Asset
# =============================================================================

@asset(
    description="NPS íˆìŠ¤í† ë¦¬ ë°ì´í„°ë¥¼ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ - Bronze Tier",
    group_name="NPS",
    kinds={"csv"},
    tags={
        "domain": "finance", 
        "data_tier": "bronze",
        "source": "national_pension"
    },
    metadata={"source": "data.go.kr"}
)
def nps_raw_ingestion(
    context: AssetExecutionContext, 
    nps_duckdb: DuckDBResource
) -> dg.MaterializeResult:
    """NPS íˆìŠ¤í† ë¦¬ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ - Fail-Fast ì •ì±…"""
    context.log.info("NPS íˆìŠ¤í† ë¦¬ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘ (Fail-Fast ì •ì±…)")
    
    PATHS.raw_history_dir.mkdir(parents=True, exist_ok=True)
    
    # ë””ìŠ¤í¬ ê³µê°„ ì‚¬ì „ ì²´í¬
    if not _check_disk_space(PATHS.raw_history_dir, CONFIG.required_disk_space_gb):
        raise RuntimeError(f"ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: ìµœì†Œ {CONFIG.required_disk_space_gb}GB í•„ìš”")
    
    max_workers = CONFIG.max_concurrent_downloads
    stop_event = threading.Event()
    successful_downloads = []
    
    # ì„¸ì…˜ ì´ˆê¸°í™”
    session = requests.Session()
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    })
    
    try:
        # Step 1: ë©”ì¸ í˜ì´ì§€ì—ì„œ PK ì¶”ì¶œ
        context.log.info("ë©”ì¸ í˜ì´ì§€ì—ì„œ PK ì¶”ì¶œ")
        main_response = session.get(URLS.main_url, timeout=CONFIG.request_timeout)
        main_response.raise_for_status()
        
        main_soup = BeautifulSoup(main_response.text, "html.parser")
        main_pk = main_soup.find(id="publicDataPk")["value"]
        detail_pk = main_soup.find(id="publicDataDetailPk")["value"]
        
        context.log.info(f"PKs ì¶”ì¶œ ì™„ë£Œ - Main: {main_pk}, Detail: {detail_pk}")
        
        # Step 2: íˆìŠ¤í† ë¦¬ ë‹¤ìš´ë¡œë“œ í›„ë³´ë“¤ ì¶”ì¶œ
        context.log.info("íˆìŠ¤í† ë¦¬ í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ë³´ë“¤ ì¶”ì¶œ")
        download_candidates = _extract_download_candidates(session, main_pk, detail_pk)
        
        # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ ë¡œê¹…
        total_candidates = len(download_candidates)
        special_candidates = [c for c in download_candidates if c.get('is_special_case', False)]
        general_candidates = [c for c in download_candidates if not c.get('is_special_case', False)]

        context.log.info(f"ğŸ“Š í›„ë³´ ë¶„ì„: ì´ {total_candidates}ê°œ")
        context.log.info(f"   ğŸ“ ì¼ë°˜ íŒŒì¼: {len(general_candidates)}ê°œ")
        context.log.info(f"   ğŸ¯ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: {len(special_candidates)}ê°œ")

        if special_candidates:
            context.log.info(f"íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ íŒŒì¼ë“¤: {[c['filename'] for c in special_candidates]}")
        else:
            context.log.info("íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ íŒŒì¼ ì—†ìŒ")

        # ì˜ˆìƒ ê°œìˆ˜ ê²€ì¦
        expected_count = 113  # ì¼ë°˜ 111 + íŠ¹ìˆ˜ 2
        if total_candidates != expected_count:
            context.log.warning(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ íŒŒì¼ ê°œìˆ˜: {total_candidates} (ì˜ˆìƒ: {expected_count})")
        else:
            context.log.info(f"ì˜ˆìƒí•œ íŒŒì¼ ê°œìˆ˜ì™€ ì¼ì¹˜: {total_candidates}ê°œ")
        
        if not download_candidates:
            raise RuntimeError("ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        context.log.info(f"{len(download_candidates)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # Step 3: ê¸°ì¡´ íŒŒì¼ë“¤ ì¡°íšŒ (ë¬´ê²°ì„± ê²€ì¦ í¬í•¨)
        context.log.info("ğŸ” ê¸°ì¡´ íŒŒì¼ ë° ë¬´ê²°ì„± ê²€ì¦ ì‹œì‘")
        with nps_duckdb.get_connection() as conn:
            existing_files, corrupted_files = _get_combined_existing_files(conn)
            
            context.log.info(f"ğŸ“Š íŒŒì¼ ìƒíƒœ ì²´í¬ ì™„ë£Œ:")
            context.log.info(f"   âœ… ìœ íš¨í•œ ê¸°ì¡´ íŒŒì¼: {len(existing_files)}ê°œ")
            context.log.info(f"   âš ï¸ ì†ìƒëœ íŒŒì¼: {len(corrupted_files)}ê°œ")
            context.log.info(f"   ğŸ“ ì´ í›„ë³´ íŒŒì¼: {len(download_candidates)}ê°œ")
            
            # ê¸°ì¡´ íŒŒì¼ ëª©ë¡ ë¡œê¹… (ì²˜ìŒ 10ê°œë§Œ, ì¤‘ë³µ ì²´í¬ ë””ë²„ê¹…ì„ ìœ„í•´)
            if existing_files:
                sample_existing = list(existing_files)[:10]
                context.log.info(f"ğŸ” ê¸°ì¡´ íŒŒì¼ ìƒ˜í”Œ (ì¤‘ë³µ ì²´í¬ìš©):")
                for i, file in enumerate(sample_existing, 1):
                    context.log.info(f"   {i}. {file}")
                if len(existing_files) > 10:
                    context.log.info(f"   ... ì™¸ {len(existing_files) - 10}ê°œ ë”")
            
            # ë‹¤ìš´ë¡œë“œ í›„ë³´ ëª©ë¡ ë¡œê¹… (ì¤‘ë³µ ì²´í¬ ë””ë²„ê¹…ì„ ìœ„í•´)
            context.log.info(f"ğŸ“‹ ë‹¤ìš´ë¡œë“œ í›„ë³´ íŒŒì¼ ìƒ˜í”Œ:")
            for i, candidate in enumerate(download_candidates[:5], 1):
                context.log.info(f"   {i}. {candidate['filename']}")
            if len(download_candidates) > 5:
                context.log.info(f"   ... ì™¸ {len(download_candidates) - 5}ê°œ ë”")
            
            # ì†ìƒëœ íŒŒì¼ë“¤ ì‚­ì œ ë° ì¬ë‹¤ìš´ë¡œë“œ ëŒ€ìƒì— ì¶”ê°€
            corrupted_filenames = []
            if corrupted_files:
                context.log.warning(f"ğŸ”§ {len(corrupted_files)}ê°œ ì†ìƒëœ íŒŒì¼ ì²˜ë¦¬ ì¤‘")
                for corrupted in corrupted_files:
                    try:
                        corrupted_path = Path(corrupted["path"])
                        if corrupted_path.exists():
                            corrupted_path.unlink()
                            context.log.warning(f"   ğŸ—‘ï¸ ì†ìƒëœ íŒŒì¼ ì‚­ì œ: {corrupted['filename']}")
                        corrupted_filenames.append(corrupted["filename"])
                    except Exception as e:
                        context.log.error(f"   âŒ ì†ìƒëœ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {corrupted['filename']} - {e}")
            
            # Step 4: ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ í•„í„°ë§ (ì‹ ê·œ + ì†ìƒëœ íŒŒì¼ë“¤)
            context.log.info("ğŸ¯ ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ í•„í„°ë§ ì¤‘")
            
            # ê°œì„ ëœ ì¤‘ë³µ ì²´í¬ ë¡œì§
            filtered_candidates = []
            skipped_files = []
            redownload_files = []
            
            for candidate in download_candidates:
                filename = candidate["filename"]
                sanitized_filename = _sanitize_filename(filename)
                if not sanitized_filename.endswith('.csv'):
                    sanitized_filename += '.csv'
                
                # ë‹¤ì–‘í•œ íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
                file_exists = any([
                    filename in existing_files,                          # ì›ë³¸ íŒŒì¼ëª…
                    sanitized_filename in existing_files,                # ì •ì œëœ íŒŒì¼ëª…
                    filename + '.csv' in existing_files,                 # í™•ì¥ì ì¶”ê°€
                    filename.replace('.csv', '') in existing_files,      # í™•ì¥ì ì œê±°
                    sanitized_filename.replace('.csv', '') in existing_files  # ì •ì œëœ íŒŒì¼ëª… í™•ì¥ì ì œê±°
                ])
                
                # ì¬ë‹¤ìš´ë¡œë“œ ì²´í¬ (ì†ìƒëœ íŒŒì¼ì¸ì§€)
                is_corrupted = any([
                    filename in corrupted_filenames,
                    sanitized_filename in corrupted_filenames
                ])
                
                if is_corrupted:
                    # ì†ìƒëœ íŒŒì¼ì€ ì¬ë‹¤ìš´ë¡œë“œ
                    filtered_candidates.append(candidate)
                    redownload_files.append(filename)
                    context.log.info(f"ğŸ”„ ì¬ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ: {filename} (ì†ìƒëœ íŒŒì¼)")
                elif not file_exists:
                    # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìƒˆ íŒŒì¼ì€ ë‹¤ìš´ë¡œë“œ
                    filtered_candidates.append(candidate)
                    context.log.debug(f"ğŸ“¥ ì‹ ê·œ ë‹¤ìš´ë¡œë“œ: {filename}")
                else:
                    # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
                    skipped_files.append(filename)
                    context.log.debug(f"â­ï¸ ê±´ë„ˆë›°ê¸°: {filename} (ì´ë¯¸ ì¡´ì¬)")
            
            # í•„í„°ë§ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
            context.log.info(f"ğŸ“‹ í•„í„°ë§ ê²°ê³¼:")
            context.log.info(f"   ğŸ“¥ ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ: {len(filtered_candidates)}ê°œ")
            context.log.info(f"   â­ï¸ ê±´ë„ˆë›¸ íŒŒì¼: {len(skipped_files)}ê°œ (ì´ë¯¸ ì¡´ì¬)")
            context.log.info(f"   ğŸ”„ ì¬ë‹¤ìš´ë¡œë“œ: {len(redownload_files)}ê°œ (ì†ìƒëœ íŒŒì¼)")
            
            context.log.info(
                f"í•„í„°ë§ ê²°ê³¼: {len(download_candidates)}ê°œ â†’ {len(filtered_candidates)}ê°œ "
                f"(ì¤‘ë³µ ì œì™¸: {len(download_candidates) - len(filtered_candidates)}ê°œ, "
                f"ì†ìƒ ì¬ë‹¤ìš´: {len(corrupted_filenames)}ê°œ)"
            )
            
            if not filtered_candidates:
                if corrupted_files:
                    context.log.info(
                        f"âœ… ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì—ˆìœ¼ë‚˜ {len(corrupted_files)}ê°œ ì†ìƒëœ íŒŒì¼ì´ ì¬ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤"
                    )
                else:
                    context.log.info("âœ… ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤")
                
                return dg.MaterializeResult(
                    metadata={
                        "status": "All files already downloaded",
                        "total_candidates": dg.MetadataValue.int(len(download_candidates)),
                        "existing_files_count": dg.MetadataValue.int(len(existing_files)),
                        "corrupted_files_found": dg.MetadataValue.int(len(corrupted_files)),
                        "corrupted_files_redownloaded": dg.MetadataValue.int(len(corrupted_filenames)),
                        "new_downloads": dg.MetadataValue.int(0)
                    }
                )
            
            # ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ë¡œê¹…
            context.log.info(f"ğŸš€ {len(filtered_candidates)}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
            if len(filtered_candidates) <= 10:
                for i, candidate in enumerate(filtered_candidates, 1):
                    is_redownload = candidate["filename"] in corrupted_filenames
                    status_icon = "ğŸ”„" if is_redownload else "ğŸ“¥"
                    context.log.info(f"   {status_icon} {i}. {candidate['filename']}")
            else:
                # ì²˜ìŒ 5ê°œì™€ ë§ˆì§€ë§‰ 5ê°œë§Œ í‘œì‹œ
                for i, candidate in enumerate(filtered_candidates[:5], 1):
                    is_redownload = candidate["filename"] in corrupted_filenames
                    status_icon = "ğŸ”„" if is_redownload else "ğŸ“¥"
                    context.log.info(f"   {status_icon} {i}. {candidate['filename']}")
                
                context.log.info(f"   ... (ì¤‘ê°„ {len(filtered_candidates) - 10}ê°œ íŒŒì¼ ìƒëµ)")
                
                for i, candidate in enumerate(filtered_candidates[-5:], len(filtered_candidates) - 4):
                    is_redownload = candidate["filename"] in corrupted_filenames
                    status_icon = "ğŸ”„" if is_redownload else "ğŸ“¥"
                    context.log.info(f"   {status_icon} {i}. {candidate['filename']}")
            
            # Step 5: ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
            context.log.info(f"âš¡ {max_workers}ê°œ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
            
            with _graceful_executor(max_workers) as executor:
                future_to_info = {
                    executor.submit(_download_single_nps_file, session, file_info, i % max_workers, stop_event): file_info
                    for i, file_info in enumerate(filtered_candidates)
                }
                
                for future in tqdm(as_completed(future_to_info), total=len(future_to_info), 
                                desc="NPS ë‹¤ìš´ë¡œë“œ", unit="files"):
                    result = future.result()
                    if result["success"]:
                        successful_downloads.append(result)
            
            # Step 5: ë©”íƒ€ë°ì´í„° ë°°ì¹˜ ì—…ë°ì´íŠ¸
            if successful_downloads:
                _update_metadata_batch(conn, successful_downloads)
        
        # ì„±ê³µ ì‹œì—ë§Œ ê²°ê³¼ ë°˜í™˜
        total_new = len(successful_downloads)
        total_size = sum(d.get("file_size", 0) for d in successful_downloads)
        
        context.log.info(
            f"ëª¨ë“  ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {total_new}ê°œ ì‹ ê·œ ë‹¤ìš´ë¡œë“œ "
            f"({total_size:,}B = {total_size / (1024**2):.1f}MB)"
        )
        
        # ì„±ëŠ¥ í†µê³„
        performance_stats = {
            "parallel_strategy": "threading",
            "max_workers_used": max_workers,
            "total_candidates": len(download_candidates),
            "successful_downloads": total_new,
            "worker_efficiency": round(total_new / max_workers, 2) if max_workers > 0 else 0,
            "avg_file_size_mb": round(total_size / (1024**2) / max(1, total_new), 2)
        }
        
        return dg.MaterializeResult(
            metadata={
                "dagster/row_count": dg.MetadataValue.int(total_new),
                "total_candidates": dg.MetadataValue.int(len(download_candidates)),
                "successful_downloads": dg.MetadataValue.int(total_new),
                "total_size_bytes": dg.MetadataValue.int(total_size),
                "total_size_mb": dg.MetadataValue.float(round(total_size / (1024**2), 1)),
                "performance_stats": dg.MetadataValue.json(performance_stats),
                "special_cases_found": dg.MetadataValue.int(len(special_candidates)),
                "general_files_found": dg.MetadataValue.int(len(general_candidates)),
                "corrupted_files_found": dg.MetadataValue.int(len(corrupted_files)) if 'corrupted_files' in locals() else 0,
                "corrupted_files_redownloaded": dg.MetadataValue.int(len(corrupted_filenames)) if 'corrupted_filenames' in locals() else 0
            }
        )
        
    except KeyboardInterrupt:
        context.log.error("í‚¤ë³´ë“œ ì¤‘ë‹¨ìœ¼ë¡œ ì¸í•œ ì‹¤í–‰ ì¤‘ë‹¨")
        stop_event.set()
        raise KeyboardInterrupt("ì‚¬ìš©ìê°€ NPS ë‹¤ìš´ë¡œë“œë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤")
        
    except Exception as e:
        context.log.error(f"NPS ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        stop_event.set()
        raise RuntimeError(f"NPS íˆìŠ¤í† ë¦¬ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}") from e
        
    finally:
        # í™•ì‹¤í•œ ì •ë¦¬
        stop_event.set()
        session.close()
        context.log.info("ì„¸ì…˜ê³¼ ëª¨ë“  ë¦¬ì†ŒìŠ¤ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

@asset(
    description="NPS íŒŒì¼ë“¤ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ê³  ì†ìƒëœ íŒŒì¼ì„ ë³´ê³  - Quality Assurance",
    group_name="NPS",
    kinds={"python"},
    tags={
        "domain": "finance", 
        "data_tier": "quality",
        "source": "national_pension",
        "validation": "integrity_check"
    },
    metadata={"validation_type": "file_integrity"}
)
def nps_file_integrity_check(
    context: AssetExecutionContext,
    nps_duckdb: DuckDBResource,
) -> dg.MaterializeResult:
    """NPS íŒŒì¼ë“¤ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ê³  ì†ìƒëœ íŒŒì¼ì„ ì‹ë³„"""
    context.log.info("NPS íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦ ì‹œì‘")
    
    if not PATHS.raw_history_dir.exists():
        context.log.warning("NPS raw history ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        return dg.MaterializeResult(
            metadata={
                "status": "Directory not found",
                "total_files_checked": 0,
                "valid_files": 0,
                "corrupted_files": 0
            }
        )
    
    # ëª¨ë“  CSV íŒŒì¼ë“¤ì„ ê²€ì¦
    all_files = list(PATHS.raw_history_dir.glob("*.csv"))
    valid_files = []
    corrupted_files = []
    
    context.log.info(f"ì´ {len(all_files)}ê°œ íŒŒì¼ì— ëŒ€í•´ ë¬´ê²°ì„± ê²€ì¦ ìˆ˜í–‰")
    
    for file_path in all_files:
        validation_result = _validate_file_integrity(file_path)
        
        if validation_result["valid"]:
            valid_files.append({
                "filename": file_path.name,
                "size_mb": validation_result["size_mb"],
                "size_bytes": validation_result["size_bytes"]
            })
        else:
            corrupted_files.append({
                "filename": file_path.name,
                "reason": validation_result["reason"],
                "details": validation_result
            })
            context.log.warning(
                f"ì†ìƒëœ íŒŒì¼: {file_path.name} - {validation_result['reason']}"
            )
    
    # ê²°ê³¼ ìš”ì•½
    total_files = len(all_files)
    valid_count = len(valid_files)
    corrupted_count = len(corrupted_files)
    
    context.log.info(f"ë¬´ê²°ì„± ê²€ì¦ ì™„ë£Œ: {valid_count}ê°œ ìœ íš¨, {corrupted_count}ê°œ ì†ìƒ")
    
    # í° íŒŒì¼ê³¼ ì‘ì€ íŒŒì¼ ë¶„ì„
    if valid_files:
        file_sizes = [f["size_mb"] for f in valid_files]
        avg_size = sum(file_sizes) / len(file_sizes)
        min_size = min(file_sizes)
        max_size = max(file_sizes)
        
        size_stats = {
            "average_size_mb": round(avg_size, 2),
            "min_size_mb": round(min_size, 2),
            "max_size_mb": round(max_size, 2),
            "total_size_gb": round(sum(file_sizes) / 1024, 2)
        }
    else:
        size_stats = {}
    
    # DuckDBì— ê²€ì¦ ê²°ê³¼ ì €ì¥
    try:
        with nps_duckdb.get_connection() as conn:
            integrity_table = f"{DATABASE.metadata_table}_integrity"
            
            create_integrity_table_sql = f"""
            CREATE TABLE IF NOT EXISTS {integrity_table} (
                check_id INTEGER PRIMARY KEY,
                filename VARCHAR,
                is_valid BOOLEAN,
                reason VARCHAR,
                size_bytes BIGINT,
                size_mb REAL,
                check_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
            conn.execute(create_integrity_table_sql)
            
            # ê¸°ì¡´ ë ˆì½”ë“œ ì‚­ì œ (ìµœì‹  ìƒíƒœ ìœ ì§€)
            conn.execute(f"DELETE FROM {integrity_table}")
            
            # ìƒˆë¡œìš´ ê²€ì¦ ê²°ê³¼ ì €ì¥
            insert_data = []
            for i, file_info in enumerate(valid_files + corrupted_files):
                is_valid = file_info in valid_files
                insert_data.append((
                    i + 1,
                    file_info["filename"],
                    is_valid,
                    file_info.get("reason", "valid") if not is_valid else "valid",
                    file_info.get("size_bytes", 0),
                    file_info.get("size_mb", 0.0)
                ))
            
            if insert_data:
                conn.executemany(f"""
                    INSERT INTO {integrity_table} 
                    (check_id, filename, is_valid, reason, size_bytes, size_mb)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, insert_data)
                
                context.log.info(f"ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼ë¥¼ DBì— ì €ì¥: {len(insert_data)}ê°œ ë ˆì½”ë“œ")
                
    except Exception as db_error:
        context.log.warning(f"DB ì €ì¥ ì‹¤íŒ¨: {db_error}")
    
    return dg.MaterializeResult(
        metadata={
            "total_files_checked": dg.MetadataValue.int(total_files),
            "valid_files": dg.MetadataValue.int(valid_count),
            "corrupted_files": dg.MetadataValue.int(corrupted_count),
            "integrity_rate_percent": dg.MetadataValue.float(
                round((valid_count / total_files) * 100, 1) if total_files > 0 else 0
            ),
            "size_statistics": dg.MetadataValue.json(size_stats) if size_stats else dg.MetadataValue.json({}),
            "corrupted_file_details": dg.MetadataValue.json(corrupted_files[:10] if corrupted_files else []),
            "sample_valid_files": dg.MetadataValue.json(valid_files[:5] if valid_files else [])
        }
    )
